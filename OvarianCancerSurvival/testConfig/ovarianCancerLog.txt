# July 21th, Wednesday, 2021
Ovarian Cancer Project Report
Outline:
1  Dataset.
2  Predictin goal.
3  challenges.
4  Tried experiments.
5  Conclusion.

https://builtin.com/machine-learning/why-do-machine-learning-projects-fail:
Such an expectation from a model is absurd because the world we live in is indeterministic.
Setting some pie-in-the-sky, out-of-this-world expectations is only going to disappoint you and your client and stop your project from going to production.

https://retina.ai/blog/7-ways-machine-learning-projects-fail/  Brad. CTO of Retina.ai   Good.
Machine learning is transforming the world. However, not all machine learning projects succeed.
Good data—that is, data which is clean, available, and relevant—is essential to machine learning.
In these cases, explanation is more important than making highly accurate predictions.
Consequently, this means data transformations — and your choices of model — must be kept simple.

Correlation is not causation. It is vital to keep this fundamental mantra from Statistics 101 in mind
 when you apply the outputs from machine learning. Machine learning finds correlations in data,
 but not direct causal relationships.

 In this case, the power of machine learning reveals hidden and unexpected correlations
 in the mountains of data you have available.


https://www.kdnuggets.com/2018/07/why-machine-learning-project-fail.html:
If you ask the wrong questions, you will get the wrong answers.
An example comes to mind from the financial industry and the problem of fraud identification.
The initial question might be “Is this particular transaction fraud or not?”.
A potentially better question to ask could be “Is this transaction anomalous or not?”.

2. Trying to use it to solve the wrong problem: Poor problem formulation: solving the wrong problem,


https://iiot-world.com/industrial-iot/connected-industry/why-85-of-machine-learning-projects-fail/
According to Gartner, 85% of Machine Learning (ML) projects fail.
(https://www.gartner.com/en/newsroom/press-releases/2018-02-13-gartner-says-nearly-half-of-cios-are-planning-to-deploy-artificial-intelligence)
Worse yet, the research company predicts that this trend will continue through 2022.

Machine learning can do remarkable things with data, but it has to be ML-ready or “clean” data.
Deep learning is like the hammer in search of nail.

When ML Is Chosen to Solve a Random Problem
However, ML isn’t good for absolutely everything.

“begin with the end in mind.”


https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/:
 Transform 2019 of VentureBeat predicted that 87% of AI projects will never make it into production.

Chris Chapo, SVP of data and analytics at Gap:
only 13% of data science projects, or just one out of every 10, actually make it into production?
“One of the biggest [reasons] is sometimes people think, all I need to do is throw money at a problem
or put a technology in, and success comes out the other end, and that just doesn’t happen,” Chapo said

https://www.lexalytics.com/lexablog/stories-ai-failure-avoid-ai-fails-2020:

Nobody believes that every AI project succeeds. Just ask MD Anderson.
Anderson blew $60 million on a Watson project before pulling the plug.
Fail: IBM’s “Watson for Oncology” Cancelled After $62 million and Unsafe Treatment Recommendations
No AI project captures the “moonshot” attitude of big tech companies quite like Watson for Oncology. In 2013, IBM partnered with The University of Texas MD Anderson Cancer Center to develop a new “Oncology Expert Advisor” system. The goal? Nothing less than to cure cancer.

The first line of the press release boldly declares, “MD Anderson is using the IBM Watson cognitive computing system for its mission to eradicate cancer.” IBM’s role was to enable clinicians to “uncover valuable insights from the cancer center’s rich patient and research databases.”

So, how’d that go?

“This product is a piece of sh–.”

In July 2018, StatNews reviewed internal IBM documents and found that IBM’s Watson was making erroneous, downright dangerous cancer treatment advice.

According to StatNews, the documents (internal slide decks) largely place the blame on IBM’s engineers. Evidently, they trained the software on a small number of hypothetical cancer patients, rather than real patient data.

The result? Medical specialists and customers identified “multiple examples of unsafe and incorrect treatment recommendations,” including one case where Watson suggested that doctors give a cancer patient with severe bleeding a drug that could worsen the bleeding.

From this Verge article:

“This product is a piece of s—,” one doctor at Jupiter Hospital in Florida told IBM executives, according to the documents. “We bought it for marketing and with hopes that you would achieve the vision. We can’t use it for most cases.”

In February 2017, Forbes reported that MD Anderson had “benched” the Watson for Oncology project. A special report from University of Texas auditors said that MD Anderson had spent more than $62 million without reaching their goals.



https://ml4devs.substack.com/p/003-why-machine-learning-projects-fail
Jan 2019: Gartner predicted that through 2020, 80% of AI projects will remain alchemy, run by wizards and through 2022, only 20% of analytic insights will deliver business outcomes.

May 2019: Dimensional Research - Alegion Survey reported 78% of AI or ML projects stall at some stage before deployment, and 81% admit the process of training AI with data is more difficult than they expected.

July 2019: VentureBeat reported 87% of data science projects never make it into production.

July 2019: International Data Corporation (IDC) survey found that a quarter of organizations reported up to 50% AI project failure rate.


https://viso.ai/computer-vision/why-computer-vision-projects-fail/:
As simple as it sounds: AI vision is not magic and cannot overcome physics.
A problem that deals with things that are not visible cannot be solved using Computer Vision.
A computer’s ability to “see” can only be as good as the image quality of the underlying camera’s video stream or a video file.


Unrealistic expectations


In most cases, the customer and vendor share responsibility for that failure. The vendor may make misleading or exaggerated claims, the customer may fail to define requirements, or the customer may not perform the necessary due diligence.

It’s easier to blame the technology, though.









# July 5th, Monday, 2021
New idea on July 5th, Monday, 2021:
1   use 2 VAEs to prepare data:
    VAE A:  input 3D volume image, and output 3D recontrcution images.
    VAE B:  input latent vector of VAE A, and output medical clinical information without reponse.
2   final 10-fold prediction:
    use the latent vector of B to predict final response.
3   the size of the latent vector of VAE B should be less than 0.4 of number of all sample, to avoid overfitting.
    the size of the latent vector of VAE A should be less than 0.4x numberOfClinicalFeature of all sample.
4   consider to use Wasserstein Auto-Encoders.


search:

https://towardsdatascience.com/compressionvae-a-powerful-and-versatile-alternative-to-t-sne-and-umap-5c50898b8696

A good dimensionality reduction technique allows us to create data that is more suitable for downstream tasks,
allowing us to build more compact models and preventing us from suffering the curse of dimensionality.

For a long time t-Distributed Stochastic Neighbor Embedding (t-SNE), and particularly its implementation
as part of scikit-learn, has been the workhorse of dimensionality reduction (along with PCA).

More recently, Uniform Manifold Approximation and Projection (UMAP) has become popular and started to supplement or
 even replace t-SNE for many applications, thanks to several advantages it offers. UMAP is considerably faster and
 scales better to high-dimensional data. It also preserves global structure better,
 making it more suitable for many applications beyond visualisation.



# May 24th, Monday, 2021
Micorsoft Azure Cloud Service.
DSVM:
https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/overview

A new idea:
1  Use VAE model to reconstruct abdominal CT images.
   Use the space compress algorithm of VAE to get size-reduced latent space,  in order to reduce overfitting.
2  the latent vector of the VAE model is regularized.
   The size of latent vector should be less than the numbers of training sample.
3  Use the latent vector to predict all possible cancer clinical label and OC and benign masses.
   Here the clinical label acts as some regualization facgtors over latent space input.

A question:
In your medical knowledge, how do determine a CT image is a benign mass, instead of an ovarian cancer?
In other words, how do get the benign mass label or ovarian cancer label for all abdominal CT images?

I think the benign mass vs ovarian caner labels are different from the surgical optimal survival result which I have worked last year.
The surgical optimal survival result is decided by 3 factors: survival 3 months, positive chemo response, and optimal cytoreduction.
I guess that it may exist a case that surgical optimal is good, but this patient dies after 6 months as it is a real ovarian cancer, instead of benign mass.
It may also exist a case that an aged patient has a benign mass, but he dies within 3 months after sugery as serious complication and side effect of the surgery.

In some sense, I think predicting the benign mass and ovarian cancer is more feasible than predicting the surgical optimal result which really is decided by a lot of extra factors.

===================================================================================================
Gonzalez Jesus Relied in May 25th, 2021:
A benign mass is decided when we do surgery and the pathologist say is benign. Is a dichotomous classification: benign vs malignant.
The problem: there is only imaging clues and symptoms before surgery that are suggestive of benign vs malignant, not a good method.
This has not changed for the last 20 years. Now we have a possibility to do so.

Thank you very much, professor Jesus.

Your answer is very informative.

It shows that the traditional method uses the pathology check of the tissue samples excised from heavy surgeries
to determine the tumor is benign or malignant. It is very expensive in medical cost and very onerous in patient's life quality.

Now, we hope to use the tumor distribution in body, texture, pattern, location, size etc information directly from the abdominal CT image without expensive surgeries to
judge whether the tumor is benign or malignant. It is very valuable in medical application.

Now I have more confidence that predicting benign or malignant tumor using deep learning is more feasible than predicting the surgical optimal result like before.

For this kind of dichotomous classification, more patient cases ,better prediction result we can get.
And we do not need tedious segmentation, just collecting more raw patient CT images and his benign and malignant label is enough.

==================================================================================================================
The choosing principle of tumor tissue samples and pathology judging criterion of benign mass or malignant ovarian cancer.

Deep learniing is not an super-machine which can do anything.
A better deep learning network always mimics domain knowledge to better locate features or ROI, especially at a small number of training cases.

The answers to belows questions will help us better design network adaptive to our application.

Further questions:
1   What is the principle of choosing tumor tissue sample after surgery to send to pathology check?
    For example, one sample from primary cancer, one sample from the metastasis, or one sample near ovaries.
    Or tumor tissue color/texture is some consideration to send to pathology check? Or something else.

2   How is a pathologist to judge a tumor tissue sample is benign and malignant?
    The pathologist just observes the tumor tissue morphology, or dose he need to do some bio-chemo experiments to culture the tissue cells and then observe the reproductive result?
    What is the major factors to let a pathologist judge a tumor tissue sample is benign or malignant?
    Size, color, texture, morphology, or something else?
========================================================================================================================

Douglas replied on May 25th, 2021:
We always send tissue for pathology of the main specimen. For instance if it is an ovarian mass that we are removing,
we will always send that ovary for pathology. We will often send other nearby structures for pathology as well if those were removed.
For instance if there's an ovarian mass removed but we do a full hysterectomy the uterus will also be sent for pathology.
Occasionally there is an additional "suspicious" piece of tissue sent for pathology - that would often be just based on
 how it visually looks during surgery. For instance it might be friable tissue or plaque-like.
 For "full staging" procedures that are performed once a primary malignancy is identified on frozen pathology
 we take the following in ovarian cancer: peritoneal cytology, multiple peritoneal biopsies, omentectomy, pelvic and para-aortic lymph node samples.

The pathologist uses their experience to check the morphology under the microscope.
This is somewhat subjective but there are specific signs and features that they are looking for that can often be subtle.
The changes are exactly as you said - size, shape, color of cells and their intracellular components.
How the cells interact with each other - for instance if it's invading surrounding areas, involving nerves, etc.
So primarily morphology - but sometimes will also do immunostaining or other molecular testing as you alluded to,
often to either to make a more specific diagnosis or when the morphology is equivocal.
It takes an entire residency to become competent in this so I'm afraid I don't have more information specifically
but maybe could put you in touch with our pathologist who does to answer this in more detail if needed.
=========================================================================================================================

Hui answer on May 26th, 2021.

n other words, the samples sent to pathology include main ovarian mass, ovary, uterus, suspicious tissues(friable tissue, plaque tissue),  peritoneum, omentum, pelvic and periaortic lymph nodes, etc.
They include broad tissues in the abdomen. The sample choice implies that the whole CT abdominal volume, instead of the primary cancer region, as input to a deep learning network is a better choice.
And a Hi-resolution volume image may help, as it needs some subtle information on the peritoneum, omentum, peri-aorta.

Now the microscope pathology will give us a big challenge. Our CT image physical resolution is 0.7mm x 0.7mm x 3 mm in general for each voxel, while the cancer cell has a size of about 10 um,
 which means one CT voxel may contain 1.47 million cancer cells (700x700x3000/(10x10x10)), which can not give us information about the cancer cell morphology, shape, interaction, invading, etc information.
 Especially, when the morphology is equivocal, more molecular tests are needed, which is unreachable by CT image information.

In other words, CT images can not offer molecular-level information which is exactly the basis of pathology analysis for benign or malignant judgement.
While CT images offer possible cancer distribution, region shape, abnormal tissue in a human-naked-eye resolution.
This information in the CT image may only offer a low-accuracy prediction to the benign or malignant prediction, as it lacks molecular-level information.

Now I have an initial prediction accuracy estimation on ovarian cancer projects, as deep learning is not a super machine exceeding human intelligence.
Project C: use pathology microscope images of ovarian mass tissues to predict benign or malignant. Directly use cell patterns and interaction to do predict. I estimate prediction accuracy can get about 80-90%.
Project B: use abdominal CT image to predict benign or malignant. Predict current status by rough CT information. I estimate the prediction accuracy can get about 60%-70%.
Project A: use abdominal CT image to predict the surgical optimal result. Predict the future by rough CT information. I estimate the prediction accuracy can get about 50-60%.

For project B, I think we need to put some effort into the medical knowledge or history medical experiments. Some questions are:
A.  Does the CT image information plus some clinical data have enough information to predict the benign or malignant tumor?
B.  What are the medical experiments or history knowledge to support the logic deducing judgment from abdominal CT images to benign/malignant judgment?
I suspect the review experts in the OCRA may also ask these questions in the grant review process.
If the answers to the above 2 questions are not solid, I suspect the prediction result will be a low-accuracy whatever we spend a lot of efforts on it.
========================================================================================================================

Jesus answered on May 27th, 2021

You are overthinking this.

The main objective would be to differentiate benign from malignant. Project B, if you will.

We will not use microscopic at this time, because there are many types of tumors (benign and malignant) and that only could complicate things without any specific gain.
=========================================================================================================================================================================
Yes. Professor Jesus. You are right that we do not use the microscope image as input at this time.

I just gave an assumed context as project C. If an assumed deep learning network taking input of tumor microscope images would directly learn the experience and knowledge of a pathologist to get a high prediction accuracy.

At this time, we can assume that abdominal CT images have similar resolution with human-naked eyes. CT image has a 3D gray 0.7mm x0.7mm x3mm resolution volume images,
while when a surgeon opens the abdomen of a woman, he can also see 3D color 1mm x1 mm x1mm resolution volume images in front of his eyes.  The advantage of CT images is we can get similar image resolution without the expensive surgery.

Now the deep learning network of our project B needs to learn and mimic the knowledge and experience of the ovarian cancer surgeon.
Professor Jesus, as an ovarian cancer expert and surgeon, your experience and knowledge on the below questions will help us better understand and design our project B network.

A.  What is your criterion with your naked eyes to judge the tumor(benign or malignant) before sending the removing samples to pathology? For example, tissue color, mass distribution, tissue texture, tumor size, or something else? More details, more helpful.
B.  According to your experience, what is the prediction accuracy of tumor (benign or malignant) just using your naked eyes, comparing with the return result from pathology?
     My intuition is that our final successful network prediction accuracy will be similar to your prediction accuracy with your naked eyes.
C.  When you use your naked eyes to judge the tumor(benign or malignant), do you use the age, BMI, blood type, blood pressure, etc clinical information in your thinking? Or what is other information for your thinking in the judgment?
   This answer will help us on how to integrate the clinical information into our network.

=============================================================================================================================================================================================================================================




























Note that we should handle 200 OC + 400 benign cases:
total 600 cases.
case size: 600x600x200  pixels,   600*600*200*4 = 300 MB
storage space: 300MB/case * 600 case/experiment *20 experiment =  4TB
memory requirement: 300 MB/case * 16 case/batch * 8   = 40GB, where 8 is considering input,network architecture of VAE


Yes. The Data Science Virtual Machine (DSVM) for Linux from Microsoft Azure will upgrade our current bottle-neck
computing resource and data access speed. As our input abdominal volume CT image has a image size of 600x600x200 in pixel,
which needs 300 MB each case for loading into memory.  Considering a batch size 16 for deep learning training,
it needs  300MB x16 = 4.7GB for input space only. Plus outpace space, network tensor space,  and back propagation space,
it needs about 40GB GPU memory for training.  We have total 600 cases, but it can not load into memory for all at one time.
The SSD offered by Azure will accelerate the batch loading speed about 10 times (1.2GB/s in SSD vs 100MB/s in HDD general),
 which will explicitly improve our training speed.

at Aug 9th, 2019, merge the 1st and 2nd batch data, excluding the non-standard data:

Statistics Information of Images:
Image Directories:
    /home/hxie1/data/OvarianCancerCT/rawNrrd/images
Totally compute 220  image files
Dimension = 3
Dimension     X      Y    Z
MinSize:    399   493   38
MaxSize:    736   736   633
MeanSize:    515   518   198
MinOrigin:    -275.7   -494.012   -937.3
MaxOrigin:    -142.198   -18.3321   1904.63
MeanOrigin:    -192.097   -280.586   56.2154
MinSpacing:    0.585938   0.585938   0.699982
MaxSpacing:    1   1   5
MeanSpacing:    0.747252   0.747255   2.90267
MinPhysicalSize:    300   300   190
MaxPhysicalSize:    502.018   508   700.247
MeanPhysicalSize:    384.633   387.387   435.725
=======End of Image Files Statistics==============




# March 27th, Saturday, 2021
=========Summary Ovarian Cancer Experiments================

==========Data==========================

=========Segmentation===================


=========Response Prediction============
