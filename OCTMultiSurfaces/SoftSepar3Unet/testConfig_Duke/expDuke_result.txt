# April 1st, Thursday, 2021
Summary previous OCT surface segmentation of
(1) BES dataset
(2) Duke dataset
I need (a) the surface positioning errors, (b) Hausdroff distances, and
(c) illustrative segmentation results of two cases for each dataset,
each including original image, ground truth, and the computed
segmentation. For the Duke dataset, please choose ADM cases.
=================================================================
For BES dataset of 10 surfaces (um):
meanError, stdError, surface0, surface1, surface2,surface3, surface4, surface5, surface6, surface7, surface8, surface9
1.993,     0.8652,   0.9598,   2.4900,   3.7846,  2.1917,   2.1953,   2.2003,   1.2041,   2.3540,   1.3879,    1.1690




# March 30th, Tuesday, 2021:
expDuke_20210322_SurfaceSubnet_M_iibi007.yaml:
     its best validaiton error: 2.243 at epoch 98

expDuke_SoftSeparation_A_20210329_iibi007:
     at epoch 18, validation Error=2.18um. It looks improve a little.

Thickness subnet result:
Thickness (Rift) prediction on Duke_AMD test data:
================================================================================================================================================================================================================
Method_Name                                         meanErr StdError thickErr0 thickErr1 HausdfDist0 HausdfDist1 Note
expDuke_20200902A_RiftSubnet                        5.9216   3.6589   3.4042,   8.4392                            Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007           3.5991   2.2252   3.0774,   4.1209    45.9394    49.9701      NoGuassian DivLoss, thickness from N surfaces, initial filter=32, 8layers
expDuke_20201215A_ThicknessSubnet_iibi007           3.8005   2.3574   3.2803,   4.3208    42.5530    54.1401      Gaussian DivLoss, thickness from N surfaces. initial filter=32, 8Layers
expDuke_20201228A_ThicknessSubnet_iibi007           3.8707   2.4398   3.2910,   4.4505    45.0367    57.6670      without Gaussian Divloss, use cumulative R, initial filter=24. 7Layers
expDuke_20210101A_ThicknessSubnet_iibi007           3.7638   2.4115   3.1838,   4.3439    52.3715    42.0491      32 filters, 8 layers, add XY coordinate input, No cummulativeRLoss
expDuke_20210101B_ThicknessSubnet_iibi007           3.8175   2.0736   3.2453,   4.3898    89.6813    27.2954      32 filters, 8 layers, add XY coordinate input, with cummulativeRLoss
expDuke_20210107A_Thickness_YufanHePlus1D_iibi007   3.6121   2.4195   3.0530,   4.1713    58.6041    42.0000      18GB, exact YufanHe network(XY channel+layerSeg+64 filter for all layers)+1D [H,1]conv
expDuke_20210107B_Thickness_1D_iibi007              4.3621   3.0423   3.3330,   5.3914    41.4189    26.6668      35GB, expDuke_20201209A network, but directly predict N-1 thickness, plus 1D [H,1] convolution.
expDuke_20210115A_Thickness_iibi007                 3.7994   2.6325   3.2553,   4.3435    90.8217    73.9762      19GB, use YufanHe's network to get N surface, and then get (N-1) thickness.
                                                                                                                        smoothSurfaceLoss = True, smoothThicknessLoss=True, lr=0.01
expDuke_20210121B_Dropout_Thickness_iibi007         3.6778   2.1588   2.9880,   4.3678    44.0114    33.7338      35GB  A added dropout N surfaces to N-1 thicknesses
                                                                                                                        smoothThicknessLoss=True, smoothSurfaceLoss=False, lr=0.1
expDuke_20210121A_JHU_Thickness_iibi007             3.7836   2.3051   3.1955,   4.3718    88.8038    40.6891      19GB  JHU network from N surfaces to N-1 thicknesses
                                                                                                                        SmoothSurfaceLoss=True, smoothThicknessLoss=True. lr=0.1
expDuke_20210121C_JHU_Thickness_NoSmooth_iibi007    3.5161   2.8313   2.9707,   4.0616    250.4738   183.8192     19GB  JHU network from N surfaces to N-1 thicknesses.
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.lr=0.1
expDuke_20210204D_Thickness_YufanHe_iibi007         3.6021   2.2906    3.0053,   4.1989    53.1821    42.            YufanHe network Plus a full Height conv of [H,1], using N surfaces without its GT, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204A_H5_Thickness_YufanHe_skm2         4.7477   2.3756    5.2286,   4.2668    129.      32.9963      YufanHe network Plus a full Height conv of [H,5], directly deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210325A_Thickness_M2_iibi007              3.4882   2.4069    2.8775,   4.0990    46.7938    52.8766     YufanHe Network(expDuke_20210121C)+ smooth Module.
=================================================================================================================================================================================================================
Notes: the above Hausdorff distances are in unit of pixel. Other error are in unit of um. 1 pixel = 3.24 um in Duke data.
=================================================================================================================================================================================================================



# March 29th, Monday, 2021:
training progress of thickness nework:
expDuke_20210325A_Thickness_M2_iibi007:
    softmax + argMax
    at epoch 60, validation error get 3.90um.  at epoch 30, validation error is 4.1um.
expDuke_20210327A_Thickness_M3_iibi007:
    mean after conv.
    at epoch 30, valiation error still get 20 um. It looks diverge.
    at epoch 50, valaition error decear some. But It still diverge.


It looks that softmax is better than [H,1] or mean over H Dimension.

expDuke_20210204D_Thickness_YufanHe_FullHeightConv_iibi007:
    at epoch 54, get valiation error of 4.0 um.



# March 27th, Saturday, 2021
Surface subnet experiment result:
Different methods on Duke_AMD test data for pure surface net, without softSeparation at all:
======================================================================================================================================================================================================
Method                      trainingGPUMemory   muError             stdError           Hausdorff_Distance(pixel)            expName
JHU(YufanHe)                18GB                1.9699755907058716  2.110976219177246  153.         40.         71.         expDuke_20201208A_SurfaceNet_YufanHe_iibi007, all layers have same filters=64
NoReLU_NoRift_ourMethod     10GB                2.0391054153442383  2.2191126346588135 65.          123.        54.         expDuke_20201117A_SurfaceSubnet_NoReLU with start filters=24
ReLU_NoRift_OurMethod(24)   10GB                1.9812164306640625  1.961887240409851  50.          33.         30.         expDuke_20200902A_SurfaceSubnet with start filters=24
ReLU_NoRift_OurMethod(64)   45GB                2.003045082092285   2.13523006439209   63.          60.         57.         expDuke_20201215A_SurfaceSubnet_iibi007 with start filters=64
0902Model+smoothModule      11GB                2.0496156215667725  2.122194290161133  71.          96.         49.         expDuke_20210322_SurfaceSubnet_M_iibi007 with filter=24
======================================================================================================================================================================================================





# March 25th, Thursday, 2021

SurfaceSubnet_M on 20210322: at epoch 81, get validation error 2.20 um with lr-0.005. This network added smooth module.
SurfaceSubnet   on 20200902A: at epoch 77, get validation error 2.23 um with  lr=0.005

expDuke_20210322A_Thickness_M_iibi007:
   at epoch 50, muThickError is 20; ErrorThickness1 keeps at 37um without decreasing. validation loss 5.9.
   [H,1] leads inconverge of surface error.

expDuke_20210204D_Thickness_YufanHe_FullHeightConv_iibi00:
  at epoch 53, muThickError is 4.08; ErrorThickness1 decrease to 4.9um. valiation loss 0.97

expDuke_20210325A_Thickness_M2_iibi007: use softmax for surfaces, and then git its thickness.
  at epoch 34, valiation loss 0.94, and meanThicknessError 4.008, ErrorThickness1 decrease to 4.622um.








# March 23rd, Tuesday, 2021:
The main program of learning lambda:  --done

# March 22nd, Monday, 2021:
1  Add smooth moudule to surface subnet, and surface subnet. --done.

# March 20th, Satursday, 2021:
Updated Model 2:
1  I added a fixed parameter 5-point moving average smooth module at final prediction surface,
   I found that in Duke_1 data, both thickness mean error and Hausdorff distance improved in both validation and test data.
2  Re-design soft separation network with a adding smooth module M;
3  Added soft separation model into constrained model:
   this soft separation model uses the consistence comparison of surface gradient and thickness gradient.
4  For details, please refer the updated attachment. I also will give you detailed report at next Tuesday meeting.


# March 19th, Friday, 2021:
Code Plan on the Model 2:
1  experiment on the predicted R: smooth it with its 5 neigbours, and then compare with ground truth;  --done
   Notes: this smooth module didn't put into training process.
   AS the result of this experiment is good, it is reasonable to add smooth operation at training network.
   muError and Hausdorff distance both improve after using smooth module which is not in training process.

ExpName: expDuke_20210204D_Thickness_YufanHe_FullHeightConv_iibi007
   for test data:
   ===============Formal Output Result  without smooth R ===========
stdThicknessError = tensor([2.4242, 1.9958], device='cuda:0')
muThicknessError = tensor([3.0053, 4.1989], device='cuda:0')
stdError = 2.29067063331604
muError = 3.6021194458007812
hausdorff distance(pixel) of Thickness = [[53.182144 42.      ]]

====Using 5-point center moving average to smooth predicted R, then compute accuracy===========
stdThicknessErrorSmooth = tensor([2.4196, 1.9861], device='cuda:0')
muThicknessErrorSmooth = tensor([3.0116, 4.1685], device='cuda:0')
stdErrorSmooth = 2.279327869415283
muErrorSmooth = 3.5900678634643555
hausdorff distance(pixel) Smooth of Thickness = [[53.092457 34.422928]]

    for validation data:
===============Formal Output Result without smooth R ===========
stdSurfaceError = tensor([2.5337, 2.8460], device='cuda:0')
muSurfaceError = tensor([3.2382, 4.6898], device='cuda:0')
stdError = 2.780116081237793
muError = 3.9639880657196045
hausdorff Distance = [[88.08188 59.     ]]

====Using 5-point center moving average to smooth predicted R, then compute accuracy===========
stdThicknessErrorSmooth = tensor([2.5298, 2.8378], device='cuda:0')
muThicknessErrorSmooth = tensor([3.2421, 4.6576], device='cuda:0')
stdErrorSmooth = 2.7694501876831055
muErrorSmooth = 3.9498488903045654
hausdorff distance(pixel) Smooth of Thickness = [[87.20867 53.60678]]

2  Code model 2 network:
   A  code lambda learning module;
   B  pretrain lambda;
   C  integrate refine 2 branch + lambda;
3  get test result;
4  use less-quantity and noise data to train;



# March 16th, Tuesday, 2021:
discussion with professor:
1  For MICCAI paper, use model of mu and sigma2 to get more robust result;
   A less training data;
   B less hausdorf distance;
   C add noise in data.
2 look into the 2nd Duke data.
3 summary Ovarian cancer previous experiments.
4 add smooth terms into model 2.




# March 14th, Sunday, 2021
For surface prediction network of expDuke_20200902A_SurfaceSubnet with filter=24:
AMD and Control group statistics for surfaceSubnet on test data:
========================================================================================================================================================
ExpName                                         Group       muError             stdError            muSurfaceError              Hausdorff_Distance(pixel)
--------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------
expDuke_20200902A_SurfaceSubnet                 AMD         2.202664852142334   2.2394497394561768  1.3282, 1.9938, 3.2859      50.     33.     30.
expDuke_20200902A_SurfaceSubnet                 Control     1.4768062829971313  0.929665207862854   0.6202, 1.6550, 2.1552      28.     8.      8.
--------------------------------------------------------------------------------------------------------------------------------------------------------
expDuke_20200902A_SurfaceSubnet               AMD+Control   1.9812164306640625  1.961887240409851   1.1122, 1.8904, 2.9410      50.     33.     30.
========================================================================================================================================================

For thickness prediction network of expDuke_20210204D_Thickness_YufanHe_iibi007 on test data:
================================================================================================================================================================================================================
Method_Name                                         meanErr StdError thickErr0 thickErr1 HausdfDist0 HausdfDist1 Note
expDuke_20210204D_Thickness_YufanHe_iibi007         3.6021  2.2906    3.0053,   4.1989    53.1821    42.
=================================================================================================================================

# March 11th, Thursday, 2021:
Lambda search result for softSeparation on validation data:
1  Surface branch: expDuke_20200902A_SurfaceSubnet with filter=24, get meanError of surface 1.9812 um;
2  Thickness branch: expDuke_20210204D_Thickness_YufanHe_iibi007, get meanError of thickness 3.6021 um.
   YufanHe network Plus a full Height conv of [H,1], using N surfaces without its GT, and then deduce N-1 thickness, lr =0.01,
   SmoothSurfaceLoss=False, smoothThicknessLoss=False.
3  The mean error of thickness branch is bigger than the mean error of surface, because of 2 reason:
   A  a surface error affect 2 thickness computations.
   B  when average, mean thickness error is divided by (N-1), and mean surface error is divided by N.
4  In our current softSeparation model, lambda is at equal position of 1/(2 simga^2).
    the coefficient of unary terms(Q = 1.0/(sigma^2)):
    Q/2 min = 5.3471991122933105e-05
    Q/2 mean = 0.6649723052978516
    Q/2 max = 5.676419734954834
    which indiates that possible lambda mean at 0.6649.
5  Grid lambda search range: lambda_0 in [0,5] with step 0.01, and lambda_1 in [0,5] with step 0.01. It searched 250K possible lambda combination.
6  The result of grid lambda search: lambda_0=0, and lambda_1=0.
   It shows that it get the best mean surface error without using pairwise terms in our current softSeparation model.

Reason Analysis and further plan:
1  The idea of using the predicted thickness(R) by another different network to improve surface location is not wrong.
   but the pairwise terms (s_{i} -s_{i-1}-r_{i-1})^2 make 2 surfaces offset ground truth as error r_{i-1} is not same with ground truth.
   And according to our observation, it is not the case that a surface offsets from ground truth wholly.
   The error case is that some points of a surface offsets ground truth, while other points fit ground truth very well. Therefore,
   a lambda value for a whole surface may hurt the accuracy, and it is better to use a lambda of size (NxW) to adapt different surface positions.
2  The reality of mean thickness error is about 2 times of mean surface error makes our current model to get the best lambda=0 by grid search.
3  Further plan: modify the softSepartion model:
      A inherit the idea of using the predicted thickness(R) by another different network to improve surface location prediction;
      B use its above and below surface and thickness prediction to get the middle surface location,
        average this surface prediction with the result of surface branch;
      C put this average surface location into a new softSeparation model without the pairwise terms, as the thickness information has merged
        into the average value.
      D The other benefit of this design is that the thickness information is under the sigma2 affecting range in the unary terms,
         which will make smaller thickness adjusting effect if sigma2 is very small.
      E optimize this new softSeparation model.
4  Pleas refer 2 attached documents:
   A: previous old model.
   B: the new whole image model: a whole image optimization model, instead a a local-column, model.




cat muErr_predictR__lmd0_0_5_0.01__lmd1_0_5_0.01_log.txt
======= Search Lambda in Soft Separation =========
sigma2Path = /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/validation/validation_sigma2_3surfaces.npy
muPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/validation/validation_result_3surfaces.npy
rPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/ThicknessSubnet_Z4/expDuke_20210204D_Thickness_YufanHe_FullHeightConv_iibi007/testResult/validation/validation_result_3surfaces.npy
gPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/validation/validation_GT_3surfaces.npy
riftGTPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/ThicknessSubnet_Z4/expDuke_20210204D_Thickness_YufanHe_FullHeightConv_iibi007/testResult/validation/validation_thicknessGT_3surfaces.npy
the coefficient of unary terms(Q = 1.0/(sigma^2)):
Q/2 min = 5.3471991122933105e-05
Q/2 mean = 0.6649723052978516
Q/2 max = 5.676419734954834
===========================

HausdorffDistance in pixel of surface branch = [[40. 64. 54.]]
HausdorffDistance in physical size (micrometer) of surface branch = [[129.6  207.36 174.96]]
muError of surface branch (lambda=0) = 2.1307780742645264
at lambda0_min= 0, and lambda1_min = 0, with ReLU, muError = 2.130784511566162
===========================

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 5, 0.01
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 5, 0.01
min location mu error with optimizaiton on 2 branches = 2.130784511566162
at min mu error,its Hausdorf distance(pixel) = [40. 64. 54.]
axis x: location: x= 0, lambda0 = 0.0
axis y: location: y= 0, lambda1 = 0.0


# March 06th, Saturday, 2021
Prepare the final optimization on surface and thickness branch.
1  surface branch: expDuke_20200902A_SurfaceSubnet with filter=24, get meanError of surface 1.9812 um;
2  thickness branch: expDuke_20210204D_Thickness_YufanHe_iibi007, get meanError of thickness 3.6021 um.
   YufanHe network Plus a full Height conv of [H,1], using N surfaces without its GT, and then deduce N-1 thickness, lr =0.01,
   SmoothSurfaceLoss=False, smoothThicknessLoss=False.
3  merge the 2 result of two branches, feed into optimization modle to get final result:
   unconstrained optimization model:
   /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/gridSearchLambda_UnconstrainedOpt_predictR.py.
   A search Lambda in validation data.
   B using searched lambda, get final result in test data.
4  work plan:
   A  use 2 trained branches, to get its validation result. (make sure validation data does not do augmentation and in order.)  --done
      surfaceNet:
      A1. modify the test code for generating validaton result.  --done
      A2. generate validation result.   --done
      output: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/validation
      and output ground truth.

      ThicknessNet:
      A1. modify the test code for generating validaton result.  --done
      A2. generate validation result.  --done
      output: /localscratch/Users/hxie1/data/OCT_Duke/numpy_slices/log/ThicknessSubnet_Z4/expDuke_20210204D_Thickness_YufanHe_FullHeightConv_iibi007/testResult/validation

   B  generate all test set output on both branches.   -done.
      surface output:
      /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/test
      thickness output:
      /localscratch/Users/hxie1/data/OCT_Duke/numpy_slices/log/ThicknessSubnet_Z4/expDuke_20210204D_Thickness_YufanHe_FullHeightConv_iibi007/testResult/test

   C  Sigma2 of surfaceNet need to save for both test and validaiton set: --done.

   D  copy all data from c-iibi007 to xwu000:   --done

   E  modify the unconstrained optimization model;  --done:


   F  search Lambda on validation data;    ----running
      search lambda0: 0-5-0.01, lambda1: 0-5-0.01   --running.
      estimated time:   0-5-1 need 1 mins, 10000min/60/24 = 7 days.

   G  code to generate the final result script;   --done
      /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/afterLambda_UnconstrainedOpt.py

   H  get final predict result on test data with searched lambda

5  further plan:
   if it get a better result on Duke data; use the this method on the Tongren data.






# Feb 11th, Thursday, 2021
Thickness (Rift) prediction on Duke_AMD test data:
================================================================================================================================================================================================================
Method_Name                                         meanErr StdError thickErr0 thickErr1 HausdfDist0 HausdfDist1 Note
expDuke_20200902A_RiftSubnet                        5.9216   3.6589   3.4042,   8.4392                            Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007           3.5991   2.2252   3.0774,   4.1209    45.9394    49.9701      NoGuassian DivLoss, thickness from N surfaces, initial filter=32, 8layers
expDuke_20201215A_ThicknessSubnet_iibi007           3.8005   2.3574   3.2803,   4.3208    42.5530    54.1401      Gaussian DivLoss, thickness from N surfaces. initial filter=32, 8Layers
expDuke_20201228A_ThicknessSubnet_iibi007           3.8707   2.4398   3.2910,   4.4505    45.0367    57.6670      without Gaussian Divloss, use cumulative R, initial filter=24. 7Layers
expDuke_20210101A_ThicknessSubnet_iibi007           3.7638   2.4115   3.1838,   4.3439    52.3715    42.0491      32 filters, 8 layers, add XY coordinate input, No cummulativeRLoss
expDuke_20210101B_ThicknessSubnet_iibi007           3.8175   2.0736   3.2453,   4.3898    89.6813    27.2954      32 filters, 8 layers, add XY coordinate input, with cummulativeRLoss
expDuke_20210107A_Thickness_YufanHePlus1D_iibi007   3.6121   2.4195   3.0530,   4.1713    58.6041    42.0000      18GB, exact YufanHe network(XY channel+layerSeg+64 filter for all layers)+1D [H,1]conv
expDuke_20210107B_Thickness_1D_iibi007              4.3621   3.0423   3.3330,   5.3914    41.4189    26.6668      35GB, expDuke_20201209A network, but directly predict N-1 thickness, plus 1D [H,1] convolution.
expDuke_20210115A_Thickness_iibi007                 3.7994   2.6325   3.2553,   4.3435    90.8217    73.9762      19GB, use YufanHe's network to get N surface, and then get (N-1) thickness.
                                                                                                                        smoothSurfaceLoss = True, smoothThicknessLoss=True, lr=0.01
expDuke_20210121B_Dropout_Thickness_iibi007         3.6778   2.1588   2.9880,   4.3678    44.0114    33.7338      35GB  A added dropout N surfaces to N-1 thicknesses
                                                                                                                        smoothThicknessLoss=True, smoothSurfaceLoss=False, lr=0.1
expDuke_20210121A_JHU_Thickness_iibi007             3.7836   2.3051   3.1955,   4.3718    88.8038    40.6891      19GB  JHU network from N surfaces to N-1 thicknesses
                                                                                                                        SmoothSurfaceLoss=True, smoothThicknessLoss=True. lr=0.1
expDuke_20210121C_JHU_Thickness_NoSmooth_iibi007    3.5161   2.8313   2.9707,   4.0616    250.4738   183.8192     19GB  JHU network from N surfaces to N-1 thicknesses.
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.lr=0.1
expDuke_20210204A_Thickness_YufanHe_iibi007         19.786  18.349    2.9211,  36.6523   48.6058     89.            YufanHe network Plus a full Height conv of [H,1], directly deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204B_Thickness_YufanHe_iibi007         19.840  18.259    3.0277,  36.6523    73.4171    89.            YufanHe network Plus a full Height conv of [H,1], using N-surface ground truth, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204C_Thickness_YufanHe_iibi007         19.870  18.239    3.0891,  36.6523   148.9094    89.            YufanHe network Plus a full Height conv of [H,1], using N surfaces, but use surface 0 ground truth only, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204D_Thickness_YufanHe_iibi007         3.6021  2.2906    3.0053,   4.1989    53.1821    42.            YufanHe network Plus a full Height conv of [H,1], using N surfaces without its GT, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204A_H5_Thickness_YufanHe_skm2         4.7477  2.3756    5.2286,   4.2668   129.        32.9963        YufanHe network Plus a full Height conv of [H,5], directly deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204B_H5_Thickness_YufanHe_skm2        19.855   18.252    3.0579,   36.6523  61.0992     89.            YufanHe network Plus a full Height conv of [H,5], using N-surface ground truth, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204C_H5_Thickness_YufanHe_skm2        19.787   18.343    2.9226,   36.6523  63.9092     89.            YufanHe network Plus a full Height conv of [H,5], using N surfaces, but use surface 0 ground truth only, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204D_H5_Thickness_YufanHe_skm2        20.872   17.301    5.0924,   36.6523  133.        89.            YufanHe network Plus a full Height conv of [H,5], using N surfaces without its GT, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.
=================================================================================================================================================================================================================
Notes: the above Hausdorff distances are in unit of pixel. Other error are in unit of um. 1 pixel = 3.24 um in Duke data.
=================================================================================================================================================================================================================

Analysis:
1  For [H,1] or [H, 5] convs overlapping on the YufanHe network, expDuke_20210204D_Thickness_YufanHe_iibi007 get the best result of muError=3.60, and minimum hausdorff distances.
   Its network: YufanHe network Plus a full Height conv of [H,1], using N surfaces without its GT, and then deduce N-1 thickness, lr =0.01
2  [H,5] conv has not explict advantage on [H,1], as [H,5] has 5 times of the number of parameters of [H,1].
3  YufanHe network(expDuke_20201208A_SurfaceNet_YufanHe_iibi007) for surface prediction can get 1.97 meanError for 3 surfaces,
   As one surface error can lead 2 thickness error, it is reasonable its mean thickness error of 2 thicknesses: (1.97*2+ 1.97*2)/2 = 3.94.
   In other words, thickness prediction error < 3.94 is a equivalent high accuracy result with its surface prediction network.

Further plan:
1  use  expDuke_20210121C_JHU_Thickness_NoSmooth_iibi007 (3.51) and  expDuke_20210204D_Thickness_YufanHe_iibi007(3.60) with surface network to get final optimal softSeparation result.






# Feb 4th, Thursday, 2021:
Thickness (Rift) prediction on Duke_AMD test data:
================================================================================================================================================================================================================
Method_Name                                         meanErr StdError thickErr0 thickErr1 HausdfDist0 HausdfDist1 Note
expDuke_20200902A_RiftSubnet                        5.9216   3.6589   3.4042,   8.4392                            Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007           3.5991   2.2252   3.0774,   4.1209    45.9394    49.9701      NoGuassian DivLoss, thickness from N surfaces, initial filter=32, 8layers
expDuke_20201215A_ThicknessSubnet_iibi007           3.8005   2.3574   3.2803,   4.3208    42.5530    54.1401      Gaussian DivLoss, thickness from N surfaces. initial filter=32, 8Layers
expDuke_20201228A_ThicknessSubnet_iibi007           3.8707   2.4398   3.2910,   4.4505    45.0367    57.6670      without Gaussian Divloss, use cumulative R, initial filter=24. 7Layers
expDuke_20210101A_ThicknessSubnet_iibi007           3.7638   2.4115   3.1838,   4.3439    52.3715    42.0491      32 filters, 8 layers, add XY coordinate input, No cummulativeRLoss
expDuke_20210101B_ThicknessSubnet_iibi007           3.8175   2.0736   3.2453,   4.3898    89.6813    27.2954      32 filters, 8 layers, add XY coordinate input, with cummulativeRLoss
expDuke_20210107A_Thickness_YufanHePlus1D_iibi007   3.6121   2.4195   3.0530,   4.1713    58.6041    42.0000      18GB, exact YufanHe network(XY channel+layerSeg+64 filter for all layers)+1D [H,1]conv
expDuke_20210107B_Thickness_1D_iibi007              4.3621   3.0423   3.3330,   5.3914    41.4189    26.6668      35GB, expDuke_20201209A network, but directly predict N-1 thickness, plus 1D [H,1] convolution.
expDuke_20210115A_Thickness_iibi007                 3.7994   2.6325   3.2553,   4.3435    90.8217    73.9762      19GB, use YufanHe's network to get N surface, and then get (N-1) thickness.
                                                                                                                        smoothSurfaceLoss = True, smoothThicknessLoss=True, lr=0.01
expDuke_20210121B_Dropout_Thickness_iibi007         3.6778   2.1588   2.9880,   4.3678    44.0114    33.7338      35GB  A added dropout N surfaces to N-1 thicknesses
                                                                                                                        smoothThicknessLoss=True, smoothSurfaceLoss=False, lr=0.1
expDuke_20210121A_JHU_Thickness_iibi007             3.7836   2.3051   3.1955,   4.3718    88.8038    40.6891      19GB  JHU network from N surfaces to N-1 thicknesses
                                                                                                                        SmoothSurfaceLoss=True, smoothThicknessLoss=True. lr=0.1
expDuke_20210121C_JHU_Thickness_NoSmooth_iibi007    3.5161   2.8313   2.9707,   4.0616    250.4738   183.8192     19GB  JHU network from N surfaces to N-1 thicknesses.
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.lr=0.1
expDuke_20210128A_Thickness_YufanHePlus1D_iibi007   validationMeanErr=159 at epoch 140, YufanHe network Plus a full Height conv of [H,5], directly deduce N-1 thickness, lr =0.1
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210128B_Thickness_YufanHePlus1D_iibi007   validationMeanErr=159 at epoch 140, YufanHe network Plus a full Height conv of [H,5], using N-surface ground truth, and then deduce N-1 thickness, lr =0.1
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210129B_Thickness_YufanHePlus1D_iibi007   validationMeanErr=159 at epoch 100, YufanHe network Plus a full Height conv of [H,5], using N surfaces, but use surface 0 ground truth only, and then deduce N-1 thickness, lr =0.1
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210129C_Thickness_YufanHePlus1D_iibi007   validationMeanErr=159 at epoch 100, YufanHe network Plus a full Height conv of [H,5], using N surfaces without its GT, and then deduce N-1 thickness, lr =0.1
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204A_Thickness_YufanHe_iibi007        training, YufanHe network Plus a full Height conv of [H,1], directly deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204B_Thickness_YufanHe_iibi007        training, YufanHe network Plus a full Height conv of [H,1], using N-surface ground truth, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204C_Thickness_YufanHe_iibi007        training, YufanHe network Plus a full Height conv of [H,1], using N surfaces, but use surface 0 ground truth only, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204D_Thickness_YufanHe_iibi007        training, YufanHe network Plus a full Height conv of [H,1], using N surfaces without its GT, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204A_H5_Thickness_YufanHe_skm2        training, YufanHe network Plus a full Height conv of [H,5], directly deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204B_H5_Thickness_YufanHe_skm2        training, YufanHe network Plus a full Height conv of [H,5], using N-surface ground truth, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204C_H5_Thickness_YufanHe_skm2        training, YufanHe network Plus a full Height conv of [H,5], using N surfaces, but use surface 0 ground truth only, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210204D_H5_Thickness_YufanHe_skm2        training, YufanHe network Plus a full Height conv of [H,5], using N surfaces without its GT, and then deduce N-1 thickness, lr =0.01
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.
=================================================================================================================================================================================================================
Notes: the above Hausdorff distances are in unit of pixel. Other error are in unit of um. 1 pixel = 3.24 um in Duke data.
=================================================================================================================================================================================================================

Analysis:
1  The above 20210128 and 2021029 4 experiments all did not converge: after 100 epochs, its validation mean error is 159, very high;
   comparing with 20210107A experiment which got 4.0 validation mean error at epoch 53.
2  There are only 2 differences between experiment 20210107A and  20210128A:
         20210107A          20210128A
   lr:   0.01               0.1
   conv: [H,1]              [H,5]
   where [H,5] convolution was hoping to capture more neighbor information along width direction to get better thickness prediction.
3  [H,1] is a 1D convolution using whole the column information to get surface location, and it is a learnable equivalence with soft_argmax in column dimension;
   [H,5] is a 2D convolution hoping to capture more neighbor information, and it offsets the idea of soft_argmax or gaussian fitting in column dimension.
4  A big-size (full height) filter of [H,1] or [H,5] may need a small learning rate as a regularization,
   otherwise the possible parameter combinations of the big-size filter have too many variation model to get an undesired local minima;
5  further improvements:
   A  stop above 4 non-converged experiments;
   B  redo above 4 experiments with new lr=0.01, and [H,1] convolution at c-iibi007.iibi.uiowa.edu
   C  redo above 4 experiments with new lr=0.01, and [H,5] convolution at skm2.healthcare.uiowa.edu











# Jan 29th, Friday, 2021
Lets consider a simple example in one column for 3 surfaces and 2 thickness:
surface 0:  fullly correct with ground truth;
surface 1:  offset 3 pixel in one column;
surface 2:  fully correct with gound truth;

Now surface mean error: 3/3 = 1 pixel
    thickness mean error: (3+3)/2 = 3 pixel, as surface 1 error leads error in 2 adjacent thickness computation.
    So it is possible that thickness mean error is bigger than surface mean error for a same prediction result.

In other words, thickness mean error in value is an augmentation value of surface mean error.
some thinking:
1  If we used surface location loss in thickness prediction, do we need to use thickness loss? does the thickness loss hurt or help?
   A. If final target is the surface location, thickness loss may hurt performance as thickness loss will force correct surface offset to get better thickness;
   B. In softSeparation model, we hope the more accurate R, better model optimization. so in the thickness network, reserving thickness loss is good.
2  in the thickness prediction network, accurate R is more important and accurate S(surface location),:
   A  use surface 0's location for single surface loss, and do not use all surface loss;
   B  and then use R thickness loss;
   This idea embeds in the experiment: expDuke_20210129B_Thickness_YufanHePlus1D_iibi007
3  basing on JHU network, plus [H,5] conv, get N surface,but do not use N-surface ground truth, directly use N-1 thickness loss:
   in experiment: expDuke_20210129C_Thickness_YufanHePlus1D_iibi007.



#Jan 27th, Wednesday, 2021

Thickness (Rift) prediction on Duke_AMD test data:
================================================================================================================================================================================================================
Method_Name                                         meanErr StdError thickErr0 thickErr1 HausdfDist0 HausdfDist1 Note
expDuke_20200902A_RiftSubnet                        5.9216   3.6589   3.4042,   8.4392                            Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007           3.5991   2.2252   3.0774,   4.1209    45.9394    49.9701      NoGuassian DivLoss, thickness from N surfaces, initial filter=32, 8layers
expDuke_20201215A_ThicknessSubnet_iibi007           3.8005   2.3574   3.2803,   4.3208    42.5530    54.1401      Gaussian DivLoss, thickness from N surfaces. initial filter=32, 8Layers
expDuke_20201228A_ThicknessSubnet_iibi007           3.8707   2.4398   3.2910,   4.4505    45.0367    57.6670      without Gaussian Divloss, use cumulative R, initial filter=24. 7Layers
expDuke_20210101A_ThicknessSubnet_iibi007           3.7638   2.4115   3.1838,   4.3439    52.3715    42.0491      32 filters, 8 layers, add XY coordinate input, No cummulativeRLoss
expDuke_20210101B_ThicknessSubnet_iibi007           3.8175   2.0736   3.2453,   4.3898    89.6813    27.2954      32 filters, 8 layers, add XY coordinate input, with cummulativeRLoss
expDuke_20210107A_Thickness_YufanHePlus1D_iibi007   3.6121   2.4195   3.0530,   4.1713    58.6041    42.0000      18GB, exact YufanHe network(XY channel+layerSeg+64 filter for all layers)+1D [H,1]conv
expDuke_20210107B_Thickness_1D_iibi007              4.3621   3.0423   3.3330,   5.3914    41.4189    26.6668      35GB, expDuke_20201209A network, but directly predict N-1 thickness, plus 1D [H,1] convolution.
expDuke_20210115A_Thickness_iibi007                 3.7994   2.6325   3.2553,   4.3435    90.8217    73.9762      19GB, use YufanHe's network to get N surface, and then get (N-1) thickness.
                                                                                                                        smoothSurfaceLoss = True, smoothThicknessLoss=True, lr=0.01
expDuke_20210121B_Dropout_Thickness_iibi007         3.6778   2.1588   2.9880,   4.3678    44.0114    33.7338      35GB  A added dropout N surfaces to N-1 thicknesses
                                                                                                                        smoothThicknessLoss=True, smoothSurfaceLoss=False, lr=0.1
expDuke_20210121A_JHU_Thickness_iibi007             3.7836   2.3051   3.1955,   4.3718    88.8038    40.6891      19GB  JHU network from N surfaces to N-1 thicknesses
                                                                                                                        SmoothSurfaceLoss=True, smoothThicknessLoss=True. lr=0.1
expDuke_20210121C_JHU_Thickness_NoSmooth_iibi007    3.5161   2.8313   2.9707,   4.0616    250.4738   183.8192     19GB  JHU network from N surfaces to N-1 thicknesses.
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.lr=0.1
expDuke_20210128A_Thickness_YufanHePlus1D_iibi007   training, YufanHe network Plus a full Height conv of [H,5], directly deduce N-1 thickness, lr =0.1
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.

expDuke_20210128B_Thickness_YufanHePlus1D_iibi007   training, YufanHe network Plus a full Height conv of [H,5], using N-surface ground truth, and then deduce N-1 thickness, lr =0.1
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.
expDuke_20210129B_Thickness_YufanHePlus1D_iibi007   training, YufanHe network Plus a full Height conv of [H,5], using N surfaces, but use surface 0 ground truth only, and then deduce N-1 thickness, lr =0.1
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.
expDuke_20210129C_Thickness_YufanHePlus1D_iibi007   training, YufanHe network Plus a full Height conv of [H,5], using N surfaces without its GT, and then deduce N-1 thickness, lr =0.1
                                                                                                                        SmoothSurfaceLoss=False, smoothThicknessLoss=False.
=================================================================================================================================================================================================================
Notes: the above Hausdorff distances are in unit of pixel. Other error are in unit of um. 1 pixel = 3.24 um in Duke data.
=================================================================================================================================================================================================================

Analysis:
1  The expDuke_20210121C experiment is the pure and exact JHU network to calculate N surfaces using surface location ground truth, and then deduce N-1 thicknesses.
   If the assumption that recognizing surface pattern is easier than recognizing thickness pattern was true, this experiment result of 3.5161 um may represent the best result for current JHU network;
2  The only difference between expDuke_20210121A and expDuke_20210121C is that expDuke_20210121A used smooth Loss on surface and thickness,
   which effectively reduces hausdorff distance, but which also adds mean error.
3  1D convolution of [H,1] in height direction effectively reduces Hausdorff distance, please refer expDuke_20210107A and expDuke_20210107B.
4  Further plan:
   A  use 3.5161 um thickness + pure surface network with softSeparation model to check its final improvement;
   B  As 1D convolution of [H,1] is not too bad (3.61um in expDuke_20210107A), use 1D convolution to directly predict N surfaces and then deduce N-1 thickness,
      it will add 1/3 ground truth using in 3-surface segmentation application, it may further improve 3.61 result with a small Hausdorff distance.
      I will implement this idea B tomorrow.






# Jan 21th, Wednesday, 2021
Thickness (Rift) prediction on Duke_AMD test data:
================================================================================================================================================================================================================
Method_Name                                         thicknessError          thicknessStdError   thickError0  thickError1  Note
expDuke_20200902A_RiftSubnet                        5.921670913696289       3.658905267715454   3.4042,      8.4392       Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007           3.5991899967193604      2.2252163887023926  3.0774,      4.1209       NoGuassian DivLoss, thickness from N surfaces, initial filter=32, 8layers
expDuke_20201215A_ThicknessSubnet_iibi007           3.800525188446045       2.357407331466675   3.2803,      4.3208       Gausssian DivLoss, thickness from N surfaces. initial filter=32, 8Layers
expDuke_20201228A_ThicknessSubnet_iibi007           3.8707656860351562      2.439850091934204   3.2910,      4.4505       without Gaussian Divloss, use cumulative R, intial filter=24. 7Layers
expDuke_20210101A_ThicknessSubnet_iibi007           3.7638235092163086      2.4115993976593018  3.1838,      4.3439       32 filters, 8 layers, add XY coordinate input, No cummulativeRLoss
expDuke_20210101B_ThicknessSubnet_iibi007           3.8175220489501953      2.0736165046691895  3.2453,      4.3898       32 filters, 8 layers, add XY coordinate input, with cumulativeRLoss
expDuke_20210107A_Thickness_YufanHePlus1D_iibi007   3.6121246814727783      2.4195966720581055  3.0530,      4.1713       18GB, exact YufanHe network(XY channel+layerSeg+64 filter for all layers)+1D [H,1]conv
expDuke_20210107B_Thickness_1D_iibi007              4.362186908721924       3.042391300201416   3.3330,      5.3914       35GB, expDuke_20201209A network, but directly predict N-1 thickness, plus 1D [H,1] convoluiton.
expDuke_20210115A_Thickness_iibi007                 3.799421787261963       2.632540702819824   3.2553,      4.3435       19GB, use YufanHe's network to get N surface, and then get (N-1) thickness.
expDuke_20210121B_Dropout_Thickness_iibi007         3.6778833866119385      2.1588995456695557  2.9880,      4.3678       35GB  A added dropout N surfaces to N-1 thicknesses
expDuke_20210121A_JHU_Thickness_iibi007             3.783660888671875       2.305124282836914   3.1955,      4.3718       19GB  JHU network from N surfaces to N-1 thicknesses, fixed the SmoothThicknessLoss error.
expDuke_20210121C_JHU_Thickness_NoSmooth_iibi007    3.5161519050598145      2.831364870071411   2.9707,      4.0616       19GB  JHU network from N surfaces to N-1 thicknesses, without SmoothThicknessLoss.
=================================================================================================================================================================================================================

Analysis:
1  expDuke_20210107B_Thickness_1D_iibi007: 13days 14hours for 496 epochs: (13*24*60+14*60)/496 = 39.4 min/epoch;  35GB GPU memory;
   directly predicting (N-1) thickness always gets a not good result.

2  expDuke_20210115A_Thickness_iibi007: 5 days 13 hours for 207 epcohs: (5*24*60+13*60)/207 = 38.6 min/epoch;  19GB GPU memory.
   I just detected an error in introducing SmoothThicknessLoss for reducing Hausdorff distance of thickness,
   in which a should-be 1st order gradient formula used an incorrect 2nd order gradient formula.  I just fixed it.

3  Now fixed error 3 networks are training. I estimate we can get their results next Wednesday.



# Jan 15th, Friday, 2021
Thickness (Rift) prediction on Duke_AMD test data:
================================================================================================================================================================================================================
Method_Name                                         thicknessError          thicknessStdError   thickError0  thickError1  Note
expDuke_20200902A_RiftSubnet                        5.921670913696289       3.658905267715454   3.4042,      8.4392       Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007           3.5991899967193604      2.2252163887023926  3.0774,      4.1209       NoGuassian DivLoss, thickness from N surfaces, initial filter=32, 8layers
expDuke_20201215A_ThicknessSubnet_iibi007           3.800525188446045       2.357407331466675   3.2803,      4.3208       Gausssian DivLoss, thickness from N surfaces. initial filter=32, 8Layers
expDuke_20201228A_ThicknessSubnet_iibi007           3.8707656860351562      2.439850091934204   3.2910,      4.4505       without Gaussian Divloss, use cumulative R, intial filter=24. 7Layers
expDuke_20210101A_ThicknessSubnet_iibi007           3.7638235092163086      2.4115993976593018  3.1838,      4.3439       32 filters, 8 layers, add XY coordinate input, No cummulativeRLoss
expDuke_20210101B_ThicknessSubnet_iibi007           3.8175220489501953      2.0736165046691895  3.2453,      4.3898       32 filters, 8 layers, add XY coordinate input, with cumulativeRLoss
expDuke_20210107A_Thickness_YufanHePlus1D_iibi007   3.6121246814727783      2.4195966720581055  3.0530,      4.1713       18GB, exact YufanHe network(XY channel+layerSeg+64 filter for all layers)+1D [H,1]conv
expDuke_20210107B_Thickness_1D_iibi007              still in training,                                                    35GB, expDuke_20201209A network, but directly predict N-1 thickness, plus 1D [H,1] convoluiton.
expDuke_20210115A_Thickness_iibi007                 training, 18GB, use YufanHe's network to get N surface, and then get (N-1) thickness.
=================================================================================================================================================================================================================

Analysis:
1  expDuke_20210107B_Thickness_1D_iibi007 is still traning whose both validation and training loss are still decreasing;
   A.  35GB GPU memory; 8d16h trained 316 epochs; (8*24*60+16*60)/316 = 40 min/epoch;
   B.  I will keep its training to next Tusday;
2  The exact JHU network didn't get better accuracy than our 20201209A network, but it is better than 20201215, 20201228, 20200101 networks;
   It shows that layerLoss helps, but less efficient than computing (N-1) thickness from N surfaces.
3  Use layerSegmentation and Surface branch to further infer thickness:
   A use the reflection point between 2 max probability values of neighbor layers to get surface location, and the infer the thickness;
   B use N surface branch to get (N-1) thickness;
   C use learning weight to average A and B result as the final result;
   D use N surface location ground truth;
   But the inferred surface location from neighbour layers is less accurate than direct surface location prediction.
   A alternative solution:
   1 use JHU's 3 channel input network;
   2 use N surfaces and layer segmentation ground truth;
   3 from N surface to deduce (N-1) thickness, and use smoothThickness loss like in 20121209A experiment;
   4 Layer segementation is helpful to improve feature extraction in Unet;
   The core idea of this network used YufanHe(JHU method) to get N surfaces and the deduce (N-1) thickness.
   Difference between YufanHe network and our surfaceNet:
   1  input channels difference: YufanHe has 3 channel, Our 8 channel;
   2  Network arichiture different:layer number 5 vs 7, filter 64x5 vs {24,48,96,192, ...]
   3  Loss function are different:











# Jan 6th, Wednesday, 2021
After U-Net, get CxHxW feature map, using 2D [H,1] convolution with pading [0,0] to get Nx1xW feature, here N is the number of thickness.

Define ThicknessSubnet_Y as yufanHe method plus 1D convolution.

Thickness (Rift) prediction on Duke_AMD test data:
=======================================================================================================================================================================================================
Method_Name                                         thicknessError          thicknessStdError   thickError0  thickError1  Note
expDuke_20200902A_RiftSubnet                        5.921670913696289       3.658905267715454   3.4042,      8.4392       Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007           3.5991899967193604      2.2252163887023926  3.0774,      4.1209       NoGuassian DivLoss, thickness from N surfaces, initial filter=32, 8layers
expDuke_20201215A_ThicknessSubnet_iibi007           3.800525188446045       2.357407331466675   3.2803,      4.3208       Gausssian DivLoss, thickness from N surfaces. initial filter=32, 8Layers
expDuke_20201228A_ThicknessSubnet_iibi007           3.8707656860351562      2.439850091934204   3.2910,      4.4505       without Gaussian Divloss, use cumulative R, intial filter=24. 7Layers
expDuke_20210101A_ThicknessSubnet_iibi007           3.7638235092163086      2.4115993976593018  3.1838,      4.3439       32 filters, 8 layers, add XY coordinate input, No cummulativeRLoss
expDuke_20210101B_ThicknessSubnet_iibi007           3.8175220489501953      2.0736165046691895  3.2453,      4.3898       32 filters, 8 layers, add XY coordinate input, with cumulativeRLoss
expDuke_20210107A_Thickness_YufanHePlus1D_iibi007   18GB, training, the exact YufanHe network(XY channel+ layerSeg+ 64 filter for all layers) + 1D [H,1] convolution to predict thickness
expDuke_20210107B_Thickness_1D_iibi007              35GB, training, base on expDuke_20201209A network, but directly predict N-1 thickness, plus 1D [H,1] convoluiton.
========================================================================================================================================================================================================

Analysis:
1  The only difference between expDuke_20210101A and expDuke_20201209A is that 20210101A added XY coordinate channels.
   Now we can figure out the value-added point of JHU method is its layer segmentation branch with dice loss and cross entropy loss;
2  cumulative loss has no value in improving thicknessError, but it improved std deviation;
3  Now the exact JHU method plus 1D convolution to directly predict N-1 thicknesses are training; I estimate we can see some results next Monday;







# Jan 6th, Wednesday, 2021
expDuke_20210101A_ThicknessSubnet_iibi007:
      at 10:50 am, Jan 6th, it trained 164 epochs in 4 days 20hours.
      it current learning rate at 0.00625, which has not reached general 0.001
      and both validation loss and training loss are in slowly decline trend.
      I will wait one more day to check its result again.


expDuke_20210101B_ThicknessSubnet_iibi007:
      at 10:59 am, Jan 6th, it trained 172 epochs in 4 day 22 hours.
      its current learning rate at 0.003125,  which has not reached general 0.001
      and both validation loss and training loss are in slowly decline trend.
      I will wait one more day to check its result again.

Today, I will make the exact JHU method + a high (different with Leixin's width,as we detect thickness instead smooth)
convolution code ready.



# Jan 5th Tuesday, 2021:
Meetting with professor:
1  use total JHU method to predict thickness in SoftSeparation proejct;
2  try Leixin's idea of using wider context to get better thickness predicion;

# Jan 1st, Friday, 2020

Thickness (Rift) prediction on Duke_AMD test data:
=======================================================================================================================================================================================================
Method_Name                                 thicknessError          thicknessStdError   thicknessError0  thicknessError1    Note
expDuke_20200902A_RiftSubnet                5.921670913696289       3.658905267715454   3.4042,          8.4392             Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007   3.5991899967193604      2.2252163887023926  3.0774,          4.1209             NoGuassian DivLoss, thickness from N surfaces, initial filter=32, 8layers
expDuke_20201215A_ThicknessSubnet_iibi007   3.800525188446045       2.357407331466675   3.2803,          4.3208             Gausssian DivLoss, thickness from N surfaces. initial filter=32, 8Layers
expDuke_20201228A_ThicknessSubnet_iibi007   3.8707656860351562      2.439850091934204   3.2910,          4.4505             without Gaussian Divloss, use cumulative R, intial filter=24. 7Layers
expDuke_20210101A_ThicknessSubnet_iibi007   training     32 filters, 8 layers, add XY coordinate input, No cummulativeRLoss
expDuke_20210101B_ThicknessSubnet_iibi007   training     32 filters, 8 layers, add XY coordinate input, with CumulativeRLoss
========================================================================================================================================================================================================

Analysis:
1  The expDuke_20201228A didn't get improvement on muError:
   One possible reason is that it used the intial filter 24 and 7 layers which are same with the best Tongren network config;
2  further improvement:
   A introduce JHU's coordinates XY as 2 extra input channels;
   B increase intial filter =32 and 48, and 8 layers for 2 experiments;
   C both new experiment networks need 35 GB GPU memory;






# Dec 25th, Friday, 2020
Record the idea for further improving the thickness Network:
1  use cumulative R to compare with cumulative G with MSE;
2  it like compare 2 cumulative curves fitting;
3  It has some shadow of comparing 2 cumulative distributions.

MSE:  \sum (r_i-g_i)^2

new: \sum (\sum_{k=0}^{k} r_k - \sum_{k=0}^{k} g_k)^2  = ((r_0- g_0) + (r_1- g_1) +...+(r_k-g_k))^2

(r_k-g_k) ^2+ 2 *(r_m-g_m)(r_n-g_n)

expDuke_20201228A_ThicknessSubnet_iibi007:
improvements:
1  add cumulativeThickness Loss;
2  first layer filter =24, and 7 layers: which is the same with the best Tongren network;
3  dropout rate change into dropoutRateUnet: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]  # for nLayers=7;
4  it needs GPU memory 11863MB.







# Dec 24th, Thursday, 2020
previous trained 2 programs:
nohup python3 ./RiftSubnet_Train.py ./testConfig_Duke/expDuke_20201215A_ThicknessSubnet_iibi007.yaml &
nohup python3 ./SurfaceSubnet_Train.py ./testConfig_Duke/expDuke_20201215A_SurfaceSubnet_iibi007.yaml &

test programs:
nohup python3 ./RiftSubnet_Test.py ./testConfig_Duke/expDuke_20201215A_ThicknessSubnet_iibi007.yaml &
nohup python3 ./SurfaceSubnet_Test.py ./testConfig_Duke/expDuke_20201215A_SurfaceSubnet_iibi007.yaml &

check result:

Pure SurfaceSubnet and Thickness prediction Result on Dec 24th, 2020

Different methods on Duke_AMD test data for pure surface net, without softSeparation at all:
======================================================================================================================================================================================================
Method                      trainingGPUMemory   muError             stdError           Hausdorff_Distance(pixel)            expName
JHU(YufanHe)                18GB                1.9699755907058716  2.110976219177246  153.         40.         71.         expDuke_20201208A_SurfaceNet_YufanHe_iibi007, all layers have same filters=64
NoReLU_NoRift_ourMethod     10GB                2.0391054153442383  2.2191126346588135 65.          123.        54.         expDuke_20201117A_SurfaceSubnet_NoReLU with start filters=24
ReLU_NoRift_OurMethod(24)   10GB                1.9812164306640625  1.961887240409851  50.          33.         30.         expDuke_20200902A_SurfaceSubnet with start filters=24
ReLU_NoRift_OurMethod(64)   45GB                2.003045082092285   2.13523006439209   63.          60.         57.         expDuke_20201215A_SurfaceSubnet_iibi007 with start filters=64
======================================================================================================================================================================================================

AMD and Control group statistics for surfaceSubnet on test data:
========================================================================================================================================================
ExpName                                         Group       muError             stdError            muSurfaceError              Hausdorff_Distance(pixel)
--------------------------------------------------------------------------------------------------------------------------------------------------------
expDuke_20201208A_SurfaceNet_YufanHe_iibi007    AMD         2.1859207153320312  2.406925916671753   1.3248, 2.0069, 3.2261      153.    40.     71.
expDuke_20201208A_SurfaceNet_YufanHe_iibi007    Control     1.478100061416626   1.0525134801864624  0.7013, 1.6296, 2.1034      92.     15.     13.
--------------------------------------------------------------------------------------------------------------------------------------------------------
expDuke_20200902A_SurfaceSubnet                 AMD         2.202664852142334   2.2394497394561768  1.3282, 1.9938, 3.2859      50.     33.     30.
expDuke_20200902A_SurfaceSubnet                 Control     1.4768062829971313  0.929665207862854   0.6202, 1.6550, 2.1552      28.     8.      8.
--------------------------------------------------------------------------------------------------------------------------------------------------------
expDuke_20201215A_SurfaceSubnet_iibi007         AMD         2.243525505065918   2.453505039215088   1.3835, 1.9919, 3.3552      63.     29.     57.
expDuke_20201215A_SurfaceSubnet_iibi007         Control     1.4552843570709229  0.9183998107910156  0.5611, 1.6485, 2.1562      19.     60.     9.
========================================================================================================================================================

Thickness (Rift) prediction on Duke_AMD test data:
=====================================================================================================================================================================================================
Method_Name                                 thicknessError          thicknessStdError   thicknessErrorRift0     thicknessErrorRift1      Note
expDuke_20200902A_RiftSubnet                5.921670913696289       3.658905267715454   3.4042,                 8.4392                   Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007   3.5991899967193604      2.2252163887023926  3.0774,                 4.1209                   without Guassian surface DivLoss, get thickness from N surfaces
xpDuke_20201215A_ThicknessSubnet_iibi007    3.800525188446045       2.357407331466675   3.2803,                 4.3208                   with Gausssian surface DivLoss, get thickness from N surfaces
=====================================================================================================================================================================================================

Analysis:
1  our latest experiments in blue:
   expDuke_20201215A_ThicknessSubnet_iibi007.yaml: uses 8 days 5 hours for training 285 epochs.
   expDuke_20201215A_SurfaceSubnet_iibi007.yaml:  uses 8 days 5hours for training 140 epochs.
2  For pure surfaceNet:
   A. it looks adding more filters does not help;
   B. new 1215 experiment has better result in Control group than 1208 experiment, but AMD group worse;
3  For Thickness prediction:
   A. it looks adding guassian N-surface divLoss odes not help;
   B. further improvement: try to introduce (N-1) thickness, intead of surface, gaussian divLoss;


#python3 ./computeAMD_ControlAccuracy.py
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201215A_SurfaceSubnet_iibi007/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([3.4635, 1.0393, 1.7873])
muSurfaceError = tensor([1.3835, 1.9919, 3.3552])
HausdorffDistance in pixel = [[63. 29. 57.]]
HausdorffDistance in physical size (micrometer) = [[204.12  93.96 184.68]]
stdError = 2.453505039215088
muError = 2.243525505065918
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.2939, 0.9639, 0.4560])
muSurfaceError = tensor([0.5611, 1.6485, 2.1562])
HausdorffDistance in pixel = [[19. 60.  9.]]
HausdorffDistance in physical size (micrometer) = [[ 61.56 194.4   29.16]]
stdError = 0.9183998107910156
muError = 1.4552843570709229
===============


python3 ./computeAMD_ControlAccuracy.py
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfacesUnet_YufanHe_2/expDuke_20201208A_SurfaceNet_YufanHe_iibi007/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([3.0343, 1.7488, 1.8714])
muSurfaceError = tensor([1.3248, 2.0069, 3.2261])
HausdorffDistance in pixel = [[153.  40.  71.]]
HausdorffDistance in physical size (micrometer) = [[495.72 129.6  230.04]]
stdError = 2.406925916671753
muError = 2.1859207153320312
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([1.0478, 1.0402, 0.4431])
muSurfaceError = tensor([0.7013, 1.6296, 2.1034])
HausdorffDistance in pixel = [[92. 15. 13.]]
HausdorffDistance in physical size (micrometer) = [[298.08  48.6   42.12]]
stdError = 1.0525134801864624
muError = 1.478100061416626
===============




# Dec 21th, Monday, 2020
python3 ./RiftSubnet_Train.py ./testConfig_Duke/expDuke_20201215A_ThicknessSubnet_iibi007.yaml:
A  further improve the thickness prediciton accuracy by adding gaussian location loss;
B  it uses 35GB GPU memory;
C  It has trained 5 days for 174 epoch,  5*24*60/174= 42 min/epoch;
D  It may need one more day to get final stable status;

=================================================================================================================
python3 ./SurfaceSubnet_Train.py ./testConfig_Duke/expDuke_20201215A_SurfaceSubnet_iibi007.yaml:
A  It adds filter number of ReLU solution to get better muError;
B  It uses 45GB GPU memory;
C  It has trained 4d 22hours for 84 epochs: (4*24*60+22*60)/84  =  84 min /epoch; It is still in training;
D  this netowrk increases filters at the begining layer from 24 to 64, and other layer same. It is almost 2.7 times bigger of 20200901A_SurfaceSubnet work;
E  Currently it just trained 84 epochs and learning just reduce one once by reduceOnPlateau , and it still needs about 3 days;


# Dec 15th, Tuesday, 2020
Meeting with professor:
1   add filter number of ReLU solution, to get better muError;  --45GB GPU memory, done; Next Tuesday get result;
2   further improve the thickness prediciton accuracy;         -- add gaussian location loss, 35GB GPU memory, done; Next Tuesday get result;

# Result of using pure SurfaceSubnet with ReLU without softSeparation:
Experiment Name: expDuke_20200902A_SurfaceSubnet
Testtime: output_20201214_201624
=====================================================================
Total Result without differentiating AMD and Control:
stdSurfaceError = tensor([2.4530, 1.0670, 1.6765], device='cuda:3')
muSurfaceError = tensor([1.1122, 1.8904, 2.9410], device='cuda:3')
stdError = 1.961887240409851
muError = 1.9812164306640625
hausdorff Distance = [[50. 33. 30.]]
pixel number of violating surface-separation constraints: 0
=====================================================================
=====================================================================
Result on AMD and Control Group:
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
========================================================================
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9140, 1.0875, 1.8950])
muSurfaceError = tensor([1.3282, 1.9938, 3.2859])
HausdorffDistance in pixel = [[50. 33. 30.]]
HausdorffDistance in physical size (micrometer) = [[162.   106.92  97.2 ]]
stdError = 2.2394497394561768
muError = 2.202664852142334
===================================================================
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4255, 1.0083, 0.4461])
muSurfaceError = tensor([0.6202, 1.6550, 2.1552])
HausdorffDistance in pixel = [[28.  8.  8.]]
HausdorffDistance in physical size (micrometer) = [[90.72 25.92 25.92]]
stdError = 0.929665207862854
muError = 1.4768062829971313
===========================================================================





# Dec 14th, Monday, 2020
JHU method result on Duke_AMD validation data: (18GB memory for training)
expName: expDuke_20201208A_SurfaceNet_YufanHe_iibi007
best error at epoch 47
muError: 2.11
muErroSurface: 0.9226, 2.266, 3.14
muStd:   2.084
muStdSurface: 1.37, 2.346, 1.804

JHU method on test Duke_AMD test data:
expName: expDuke_20201208A_SurfaceNet_YufanHe_iibi007
stdSurfaceError = tensor([2.5991, 1.5675, 1.6567], device='cuda:1')
muSurfaceError = tensor([1.1345, 1.8918, 2.8836], device='cuda:1')
stdError = 2.110976219177246
muError = 1.9699755907058716
hausdorff Distance = [[153.  40.  71.]]
pixel number of violating surface-separation constraints: 0

Different methods on Duke_AMD test data for pure surface net, without softSeparation at all:
============================================================================================================================================================================
Method                      trainingGPUMemory   muError             stdError           Hausdorff_Distance(pixel)            expName
JHU                         18GB                1.9699755907058716  2.110976219177246  153.         40.         71.         expDuke_20201208A_SurfaceNet_YufanHe_iibi007
NoReLU_NoRift_ourMethod     10GB                2.0391054153442383  2.2191126346588135 65.          123.        54.         expDuke_20201117A_SurfaceSubnet_NoReLU (newTest)
ReLU_NoRift_OurMethod       10GB                1.9812164306640625  1.961887240409851  50.          33.         30.         expDuke_20200902A_SurfaceSubnet (newTest)
=============================================================================================================================================================================


==================================================================================

cmd: python3 ./RiftSubnet_Train.py ./testConfig_Duke/expDuke_20201209A_ThicknessSubnet_iibi007.yaml
New ThicknessNetwork for  on Duke_AMD validation data: (34GB memory for trainig)
expName: expDuke_20201209A_ThicknessSubnet_iibi007
best error at epoch 112
muError: 4.036
muErroSurface: 3.495, 4.646
muStd:   2.725
muStdSurface: 2.663, 2.689


comparing with history:
RiftSubnet For test data:
experimentName:expDuke_20200902A_RiftSubnet: for Rift prediction:
muError = 5.921670913696289
muSurfaceError = tensor([3.4042, 8.4392], device='cuda:2')
stdError = 3.658905267715454
stdSurfaceError = tensor([2.7749, 2.5322], device='cuda:2')
Lr = 1.0e-2
epoch = 42  for training.

===========================================
===============Formal Output Result ===========
experimentName:expDuke_20201209A_ThicknessSubnet_iibi007
stdThicknessError = tensor([2.5041, 1.7789], device='cuda:0')
muThicknessError = tensor([3.0774, 4.1209], device='cuda:0')
stdError = 2.2252163887023926
muError = 3.5991899967193604
hausdorff distance of Thickness = [[45.939484 49.970154]]

%%%%%%%%%%%%%%%%%%%%%%%%%

Thickness (Rift) prediction on Duke_AMD test data:
====================================================================================================================================
Method_Name                                 thicknessError          thicknessStdError   thicknessErrorRift0     thicknessErrorRift1
expDuke_20200902A_RiftSubnet                5.921670913696289       3.658905267715454   3.4042,                 8.4392
expDuke_20201209A_ThicknessSubnet_iibi007   3.5991899967193604      2.2252163887023926  3.0774,                 4.1209
=====================================================================================================================================

Analysis:
1  current ThicknessPrediction Network(expDuke_20201209A_ThicknessSubnet_iibi007) didn't use surfaceLocation loss;
2  further improvement: add surfacelocation gaussian loss;
3  risk: adding surfaceLocation guasssian loss may improve muError, but it may reduce differentiating information from surfacenet;
         but it ThicknessNetwork has different network architecture, it may be still has differentiating information;






# Dec 9th, Wednesday, 2020
Further think the improvement of RiftNet:
idea of further improving rift(thickness) network:
  A  Inherit the technologies of our current U-net predicting N surfaces with high accuracy;
  B  First use a separate U-Net to output N-surface location probabilities, and then compute its (N-1) thickness(R) from this N-Surfaces location probability;
  C  Compare this (N-1) thickness with ground truth R to compute loss and backward propagation;
  D  This network only uses (N-1) thickness ground truth, without using N surface ground truth at all;
     As ground truths and loss functions, network initialization are not same, riftNetwork and surfaceNetwork are different networks.
  F  This method fits physical intuition of layer thickness computation;
  G  Add dropout in the UNet After concatenation in the expand path;   --done
  H  Add filter number of each layer, as predicting thickness is more complicated than predicting surfaces;  --done
  I  Thickness ground truth does not need smooth;   --done
  G  Use L1 loss + smoothThicknessLoss; --done
  I  initial layer filter number =32, which is different with surfaceNet;  --done

Now new thickness network  in training.

Duke_AMD data is 12.7 times bigger than Tongren data,
and we can not load all of them into memory one time, so each epoch, we need load data from the disk.
and both new network has bigger filter number.
The new JHU network and Thickness Network both need 45 min per epoch.

Duke_AMD data statistics:
  training set: 266 volumes x 51 slices per volume, where 187 AMD + 79 control = 266 volumes;
  validation set: 59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;
  test set:  59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;



# Dec 07th, Monday, 2020
The result of grid searching lambda for gaussian noise + perfect thickness(R) on validation data:

Summary:
1 Unconstrained soft separation optimization + ReLU do improves muError; and its training speed is very quick, comparing with IPM;
2 Higer accuracy of thickness prediction, bigger lambda values, less muError;
3 From the below Guassian noise + perfect R experiments comparing with NoReLU, No optimization,
  it improves Hausdorff distance significantly.
4 idea of further improving rift(thickness) network:
  A  Inherit the technologies of our current U-net predicting N surfaces with high accuracy;
  B  First use a separate U-Net to output N-surface location probabilities, and then compute its (N-1) thickness(R) from this N-Surfaces location probability;
  C  Compare this (N-1) thickness with ground truth R to compute loss and backward propagation;
  D  This network only uses (N-1) thickness ground truth, without using N surface ground truth at all;
     As ground truths and loss functions are not same, riftNetwork and surfaceNetwork are different networks.
  F  This method fits physical intuition of layer thickness computation;


Some tabular details:
HausdorffDistance in pixel for NoReLU and No optimization = [[38.474457 66.437805 68.57501 ]]
HausdorffDistance in physical size (micrometer) for NoReLU and No optimization = [[124.65724 215.25848 222.18304]]
muError at NoReLU and No optimiztion (lambda=0) = 2.383660316467285
at lambda0_min= 0, and lambda1_min = 0, with ReLU, muError = 2.383739471435547

# below the best result of grid searching lambda for guassian noise + perfect thickness(rift) on validation data:
# blank means no measurement in that programs.
-------------------------------------------------------------------------------------------------------------------------
RiftError           muError                 HausdorffDistance(in pixel)     Lambda_0 Lambda_1
-------------------------------------------------------------------------------------------------------------------------
0.0                 1.6399389505386353                                      2.06     3.99   (this perfect R case)
2.197073459625244   1.90798819065094    49.196      52.69426    39.68515    0.1      0.25   (gaussian noise + perfect R)
2.4565892219543457  1.9342775344848633  49.261276   53.30031    37.779327   0.08     0.2    (gaussian noise + perfect R)
2.7137765884399414  1.9585803747177124  48.728424   54.165863   35.518845   0.07     0.16   (gaussian noise + perfect R)
2.972208023071289   1.9796805381774902  49.18547    54.445984   34.13138    0.06     0.14   (gaussian noise + perfect R)
5.921670913696289   2.3492398262023926                                      0.004    0.004  (current predicted R case)
NoRift/No Optimize  2.383660316467285   38.474457   66.437805   68.57501    0.0      0.0    (No ReLU, No optimization)
-------------------------------------------------------------------------------------------------------------------------

# Dec 4th, Friday, 2020
Ideas for further improving R:
1  Use dense network architecture;
2  Use U-Net to predict N surface, and then get N-1 layer thickness to compare with ground truth;


# Dec 2nd, Wednesday, 2020
Goal:
   Before improving the RiftSubnet, Let's add gaussian noises to perfect ground truth R, to see what kind of accuracy we can get with grid search lambda:
Design:
1  add 4 different gaussian noises to perfect ground truth R, to search its best lambda separately;
2  alos measure haudorff distance in grid search;   -- done

Subtasks:
1  add gaussian noise, generate its numpy array and save to harddisk file;  --done
python3 ./generateGaussianNoiseGTRift.py
Gaussian noise sigma = 0.85 for below rift(thickness) error:
	stdRiftError = tensor([0.0127, 0.0122], device='cuda:0')
	RiftError = tensor([2.1980, 2.1961], device='cuda:0')
	stdRiftError = 0.012410839088261127
	riftError = 2.197073459625244
========================
Gaussian noise sigma = 0.95 for below rift(thickness) error:
	stdRiftError = tensor([0.0156, 0.0115], device='cuda:0')
	RiftError = tensor([2.4568, 2.4564], device='cuda:0')
	stdRiftError = 0.013624467886984348
	riftError = 2.4565892219543457
========================
Gaussian noise sigma = 1.05 for below rift(thickness) error:
	stdRiftError = tensor([0.0178, 0.0169], device='cuda:0')
	RiftError = tensor([2.7154, 2.7121], device='cuda:0')
	stdRiftError = 0.01734689623117447
	riftError = 2.7137765884399414
========================
Gaussian noise sigma = 1.15 for below rift(thickness) error:
	stdRiftError = tensor([0.0197, 0.0172], device='cuda:0')
	RiftError = tensor([2.9714, 2.9730], device='cuda:0')
	stdRiftError = 0.01843925192952156
	riftError = 2.972208023071289
========================

2  modifiy main program to support noiseRiftGT to search 4 ranges:  --Runing. wait to result.
   lambda0_min, lambda0_max, lambda0_step = 0, 5.0, 1.0e-2
   lambda0_min, lambda0_max, lambda0_step = 0, 5.0e-1, 2.0e-3
   lambda0_min, lambda0_max, lambda0_step = 0, 4.0e-3, 2.0e-5
   lambda0_min, lambda0_max, lambda0_step = 0, 4.0e-7, 2.0e-9










# Nov 30th, Monday, 2020
Grid search Lambda with unconstrained optimziaiton +ReLU on the validation data:
Summary:
1  at lambda=0 (which means not using soft separation) on validation data, muError = 2.38373947;
2  In the case of using perfect R from ground truth, lambda0=2.06, lambda1=3.99, muError=1.6399;
   Reflecction:
   A  The coefficient unary terms is in [0.000017, 3.492] with mean 0.4896; does this big lambda make sense in physical meaning?
   B  If lambda0= 0.004, lambda1=0.004 at predicted R case level, the muError = 2.24765 with the perfect R, improving 5.7% comparing with  no softSeparation;
   C  Previous binary search experiments didn't search so big lambda value.
3  In the case of using predicted R from another network, lambda0=0.004, lambda1=0.004, muError = 2.349234;
   Comparing with the No soft separation case, this muError improves 1.4%;
   Reflection:
   A when R is not accurate, improvement is not explict;
   B We need to further improve predict R; we need to rethink the architecture of predictR network to improve it;
   C A caveat: The accuracy of R is more difficult to achieve the same level with mu,as R involves 2 measured surfaces;
4  Searching lambda with constrained IPM, and searching Lambada with unconstrained optimization may have different behaviors;

Grid search Lambda with unconstrained optimziaiton +ReLU:
sigma2Path = /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201117A_SurfaceSubnet_NoReLU/testResult/validation/validation_simga2.npy
muPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201117A_SurfaceSubnet_NoReLU/testResult/validation/validation_mu.npy
rPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/RiftSubnet/expDuke_20200902A_RiftSubnet/testResult/validation/validation_Rift.npy
gPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201117A_SurfaceSubnet_NoReLU/testResult/validation/validation_gt.npy
riftGTPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/RiftSubnet/expDuke_20200902A_RiftSubnet/testResult/validation/validation_RiftGts.npy
the coefficient of unary terms(Q = 1.0/(sigma^2)):
Q/2 min = 0.000174444867298007
Q/2 mean = 0.48962265253067017
Q/2 max = 3.4920814037323
output Dir = /home/hxie1/data/OCT_Duke/numpy_slices/searchSoftLambda

at both min lambdas (=0), MuError= 2.383739471435547
===============================
Use predicted R for grid-searching Lambda:

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4.0, 0.01
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4.0, 0.01
min error location with mu error = 2.356391668319702:
axis x: location: x= 1, lambda0 = 0.01
axis y: location: y= 1, lambda1 = 0.01

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 0.5, 0.002
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 0.5, 0.002
min error location with mu error = 2.3492398262023926: (Best)
axis x: location: x= 2, lambda0 = 0.004
axis y: location: y= 2, lambda1 = 0.004

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 0.004, 2e-05
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 0.004, 2e-05
min error location with mu error = 2.3492515087127686:
axis x: location: x= 199, lambda0 = 0.00398
axis y: location: y= 199, lambda1 = 0.00398

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4e-05, 2e-07
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4e-05, 2e-07
min error location with mu error = 2.3812191486358643:
axis x: location: x= 200, lambda0 = 3.9999999999999996e-05
axis y: location: y= 200, lambda1 = 3.9999999999999996e-05

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4e-07, 2e-09
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4e-07, 2e-09
min error location with mu error = 2.383711099624634:
axis x: location: x= 190, lambda0 = 3.8e-07
axis y: location: y= 198, lambda1 = 3.96e-07

==============================
Use perfect ground truth R for grid-searching Lambda:

rSource = GTR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4.0, 0.01
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4.0, 0.01
min error location with mu error = 1.6399389505386353:  (Best)
axis x: location: x= 206, lambda0 = 2.06
axis y: location: y= 399, lambda1 = 3.99

rSource = GTR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 0.5, 0.002
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 0.5, 0.002
min error location with mu error = 1.695436954498291:
axis x: location: x= 249, lambda0 = 0.498
axis y: location: y= 249, lambda1 = 0.498

rSource = GTR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 0.004, 2e-05
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 0.004, 2e-05
min error location with mu error = 2.247654676437378:
axis x: location: x= 199, lambda0 = 0.00398
axis y: location: y= 199, lambda1 = 0.00398

rSource = GTR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4e-05, 2e-07
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4e-05, 2e-07
min error location with mu error = 2.3794524669647217:
axis x: location: x= 200, lambda0 = 3.9999999999999996e-05
axis y: location: y= 200, lambda1 = 3.9999999999999996e-05

rSource = GTR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4e-07, 2e-09
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4e-07, 2e-09
min error location with mu error = 2.383693218231201:
axis x: location: x= 198, lambda0 = 3.96e-07
axis y: location: y= 198, lambda1 = 3.96e-07

=================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# Nov 26th, 2020: Unary terms coefficient:
Q = 1/(sigma^2) on validation data:
Q/2 min = 0.000174444867298007
Q/2 mean = 0.48962265253067017
Q/2 max = 3.4920814037323

# Nov 25th, 2020  expDuke_20201117A_SurfaceSubnet_NoReLU Result
expDuke_20201117A_SurfaceSubnet_NoReLU finished:
1  trained 372 epochs in 7day 21hour 12min:
   (7*24*60+21*60+12)/372 = 30.5 min / epoch;
2  best validation error:
   at epoch 85: with learning rate 5.0e-3
   meanError: 2.207
                Error      Std
   surface 0:   0.9427     1.259
   surface 1:   2.371      2.588
   surface 2:   3.306      1.814
3  test on test data:
=======net running parameters=========
B,S,H,W = (3009, 3, 512, 361)
Test time: 204.82324028015137 seconds.
net.m_runParametersDict:
	validationLoss:40.726619720458984
	epoch:76
	errorMean:2.205378532409668
	learningRate: 5.0e-3

===============Formal Output Result ===========
patientIDList =[]
stdSurfaceError = tensor([3.0907, 1.1151, 1.5977], device='cuda:3')
muSurfaceError = tensor([1.2110, 1.9339, 2.9723], device='cuda:3')
stdError = 2.21911358833313
muError = 2.039097547531128
pixel number of violating surface-separation constraints: 67
slice number of violating surface-separation constraints: 5
slice list of violating surface-separation constraints:
	/home/hxie1/data/OCT_Duke/numpy_slices/test/AMD_1134_images_s29.npy
	/home/hxie1/data/OCT_Duke/numpy_slices/test/AMD_1257_images_s39.npy
	/home/hxie1/data/OCT_Duke/numpy_slices/test/AMD_1257_images_s41.npy
	/home/hxie1/data/OCT_Duke/numpy_slices/test/AMD_1257_images_s42.npy
	/home/hxie1/data/OCT_Duke/numpy_slices/test/AMD_1105_images_s44.npy
==================================================
test on Divided AMd and Control Group without ReLU:
python3.7 ./computeAMD_ControlAccuracy.py
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201117A_SurfaceSubnet_NoReLU/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([3.6814, 1.1590, 1.7918])
muSurfaceError = tensor([1.4751, 2.0601, 3.3228])
HausdorffDistance in pixel = [[ 65. 123.  54.]]
HausdorffDistance in physical size (micrometer) = [[210.6  398.52 174.96]]
stdError = 2.5565528869628906
muError = 2.2859814167022705
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.3893, 0.9779, 0.4277])
muSurfaceError = tensor([0.6097, 1.6466, 2.1740])
HausdorffDistance in pixel = [[46. 13.  9.]]
HausdorffDistance in physical size (micrometer) = [[149.04  42.12  29.16]]
stdError = 0.9188244938850403
muError = 1.4767508506774902
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
RiftSubnet For test data:
experimentName:expDuke_20200902A_RiftSubnet: for Rift prediction:
stdSurfaceError = tensor([2.7749, 2.5322], device='cuda:2')
muSurfaceError = tensor([3.4042, 8.4392], device='cuda:2')
stdError = 3.658905267715454
muError = 5.921670913696289

Lr = 1.0e-2
epoch = 42  for training.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Search best Lambda on validation data with unconstrained soft separation:
1  use riftNet: "/home/hxie1/Projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20200902A_RiftSubnet.yaml"
   and SurfaceNet Without ReLU:expDuke_20201117A_SurfaceSubnet_NoReLU
2  basic design:
   A use unconstrained softSeparation and ReLU at the last to estimate Lambda;
   B use unconstrained softSeparation optimal solution get S;
   C use validation data, without augmentation, output all its Q, mu, r:
     Q: Bx51x3x361
     mu:Bx51x3x361
     r: Bx51x2x361,
     g: Bx51x3x361,
     where B is the number of OCT volumes of validation data;
3  some subtasks:
   A generate Q,mu,r,g in validation data; -- done;
      surfaceResult: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201117A_SurfaceSubnet_NoReLU/testResult/validation
      riftResult: /home/hxie1/data/OCT_Duke/numpy_slices/log/RiftSubnet/expDuke_20200902A_RiftSubnet/testResult/validation
      And checked its order are consistent.
   B code the unconstrained optimal function;  --done
   C grid search lambda with generated Q,mu, r,g on validation data; -- on going. Monday it will have initial result.
     2 experiments parallel both on validation data:
     A  use predicted R;
     B  use ground truth R;
     1 search needs one second.
      lambda0_min, lambda0_max, lambda0_step = 0, 4.0, 0.01
      lambda1_min, lambda1_max, lambda1_step = 0, 4.0, 0.01,
      grid search totally needs 400*400 seconds = 44.4 hours. On Monday it will have initial result.

   D test expDuke_20201117A_SurfaceSubnet_NoReLU on test data;  --done, with average error 2.039 um.;
   E test searched fixed lambda fine tune on validation data with 2 experiments;   -- plan on Monday
      A  fixed Lambda + unconstrained soft separation optimization+ ReLU;
      B  fixed Lambda + IPM;
   F test searched fixed lambda final on test data with 2 experiments;             --plan on Wednesday
      A  fixed Lambda + unconstrained soft separation optimization + ReLU;
      B  fixed Lambda + IPM;

# Nov 19th, 2020
Exact slices with max hausdorff distance on Duke_AMD test data.

Summary:
1   These slices represent the worst error in the test data;
    Note: Hausdorff distance is a measurement relating to specific cases.
2   check error one by one:
    AMD_1198_images_s24_GT_Predict:  a rare disease case that surface 0 broken in the fovea region;
    AMD_1127_images_s23_GT_Predict:  surface 1 prediction error. By the way: the ground truth of surface 0 is not good;
    AMD_1198_images_s17_GT_Predict:  surface 2 prediction error;
    Control_1051_images_s16_GT_Predict: surface 0 signal is blurry, leading to prediction error;
    Control_1005_images_s24_GT_Predict: surface 1 prediction error;
    Control_1016_images_s08_GT_Predict: explicit ground truth error;
3   Surface 0 has maximum Hausdorff distance because the AMD_11198 case in the test data is a rare ILM broken patient;
4   Please refer to the attachments.

Search the worst cases: the slices with max hausdorff distance.
######################################################
Use softSeparation + fixed lambda network on Test data:
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
HausdorffDistance in pixel = [[50. 33. 31.]]
HausdorffDistance in physical size (micrometer) = [[162.   106.92 100.44]]
surface 0: the location of hausdorff at w=179 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/AMD_1198_images_s24_GT_Predict.png
surface 1: the location of hausdorff at w=126 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/AMD_1127_images_s23_GT_Predict.png
surface 2: the location of hausdorff at w=341 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/AMD_1198_images_s17_GT_Predict.png
===============
GroupName: Control
case number = 18
HausdorffDistance in pixel = [[28.  8.  8.]]
HausdorffDistance in physical size (micrometer) = [[90.72 25.92 25.92]]
surface 0: the location of hausdorff at w=71 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/Control_1051_images_s16_GT_Predict.png
surface 1: the location of hausdorff at w=171 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/Control_1005_images_s24_GT_Predict.png
surface 2: the location of hausdorff at w=132 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/Control_1016_images_s08_GT_Predict.png
===============
######################################################
Use Unet + ReLU network on Test data
python3.7 ./findMaxHausdorffPoint.py
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
HausdorffDistance in pixel = [[50. 31. 44.]]
HausdorffDistance in physical size (micrometer) = [[162.   100.44 142.56]]
surface 0: the location of hausdorff at w=179 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/AMD_1198_images_s24_GT_Predict.png
surface 1: the location of hausdorff at w=360 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/AMD_1100_images_s48_GT_Predict.png
surface 2: the location of hausdorff at w=360 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/AMD_1100_images_s48_GT_Predict.png
===============
GroupName: Control
case number = 18
HausdorffDistance in pixel = [[33.  9.  8.]]
HausdorffDistance in physical size (micrometer) = [[106.92  29.16  25.92]]
surface 0: the location of hausdorff at w=48 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/Control_1051_images_s17_GT_Predict.png
surface 1: the location of hausdorff at w=171 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/Control_1005_images_s24_GT_Predict.png
surface 2: the location of hausdorff at w=132 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/Control_1016_images_s08_GT_Predict.png
===============
######################################################


# Nov 19th, 2020
Compute Hausdorff distance and segmentation error computation on AMD and Control test data separately.
Summary:
Compare pure Unet+ReLu and SoftSeparation Network:
1  In AMD test data, surface 2 improves about 30% of Hausdorff distance; while surface 0 didn't improve, and surface 1 worsen a little;
   We can use this surface 2 as an example for a demonstration;
2  In Control test data, surface 0 and surface 1 improve a little, while surface 3 didn't change.
   The reason behind it maybe is that control test data already has a high accuracy; (High accuracy improves small);
3  Hausdorff distance improves really depending on data.
4  In total result, SoftSeparation improves both Hausdorff distance, and mean accuracy;
5  Unet without ReLU is still in training. After its training, we may further compare it with SoftSeparation network;
   This comparison will be more explicit in our estimation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately in SoftSeparation Network + fixed Lambda
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9144, 1.0877, 1.8948])
muSurfaceError = tensor([1.3283, 1.9940, 3.2859])
HausdorffDistance in pixel = [[50. 33. 31.]]
HausdorffDistance in physical size (micrometer) = [[162.   106.92 100.44]]
stdError = 2.239591121673584
muError = 2.202744960784912
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4251, 1.0081, 0.4460])
muSurfaceError = tensor([0.6201, 1.6549, 2.1552])
HausdorffDistance in pixel = [[28.  8.  8.]]
HausdorffDistance in physical size (micrometer) = [[90.72 25.92 25.92]]
stdError = 0.9295611381530762
muError = 1.4767508506774902
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately in Unet + ReLU
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9164, 1.0329, 2.0026])
muSurfaceError = tensor([1.3764, 1.9611, 3.3444])
HausdorffDistance in pixel = [[50. 31. 44.]]
HausdorffDistance in physical size (micrometer) = [[162.   100.44 142.56]]
stdError = 2.267122268676758
muError = 2.2273151874542236
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4579, 1.1336, 0.4604])
muSurfaceError = tensor([0.6594, 1.7114, 2.1802])
HausdorffDistance in pixel = [[33.  9.  8.]]
HausdorffDistance in physical size (micrometer) = [[106.92  29.16  25.92]]
stdError = 0.9794862270355225
muError = 1.516985535621643
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


# Nov 17th, 2020
Compute Accuracy separately in AMD and Control of Duke_AMD data:
Analysis:
1  Comparing ReLU constraints with SoftSepartion constraints, control group accuracy improves more with SoftSeparation;
2  Control group improves 0.04, while AMD group improves 0.02;
   So I have some suspicion that more disease data may will get better result.
3  Comparing our result with Leixin's result on the Table 1 of https://arxiv.org/pdf/2007.01217.pdf;
   our surface 3 corresponds with OBM, our surface 2 correpondis with IRPE. Our Control correspond its normal.
   Both our result in both control and AMD groups are better than Leixin's best result "W/".
   Leixin's used neighbor smooth constraints, while our method used soft separation constraints.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately in expDuke_20201113A_FixLambda2Unet Network:
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9144, 1.0877, 1.8948])
muSurfaceError = tensor([1.3283, 1.9940, 3.2859])
stdError = 2.239591121673584
muError = 2.202744960784912
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4251, 1.0081, 0.4460])
muSurfaceError = tensor([0.6201, 1.6549, 2.1552])
stdError = 0.9295611381530762
muError = 1.4767508506774902
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately in pure SurfaceSubnet with ReLU constraint without SoftSeparation:
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9164, 1.0329, 2.0026])
muSurfaceError = tensor([1.3764, 1.9611, 3.3444])
stdError = 2.267122268676758
muError = 2.2273151874542236
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4579, 1.1336, 0.4604])
muSurfaceError = tensor([0.6594, 1.7114, 2.1802])
stdError = 0.9794862270355225
muError = 1.516985535621643
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# Nov 17th, 2020
Meeting with professor:
1  we need to use JHU method to test Duke_AMD data;
2  make sure JHU method same with its paper, without using our gradient;
3  JHU's method has a Journel verson;

My think:
1  Leixin's method is single surface optimization, so its lambda only fit its single surface;
2  Our method is multi-surface paralllel optimization, so its final lambda is the balance of multi-surface;
3  we may need a special disease for show our method;
4  soft separation cost function does not consider the lambda for top and bottom surfaces.

# Nov 16th, 2020
Response to professor Wu:
 "I'd suggest you to calculate the segmentation accuracy for the normal and AMD subjects separately. I am still puzzling on the tiny \lambda."
 "We should compare to the model without any separation constraints. Also please compute the segmentation accuracy for the normal and AMD subjects separately.

1  OK.  I will write a script to divide normal and AMD case, and statistics its accuracy separately in test set;
2  "Compare model without any separation constraints" is a wonderful/insightful idea; Thank you,professor. ReLU is a hard constraint.
    OK. I need to retrain a network to test it;
3  I am totally not surprise this tiny lambda result:
   A. Grid searching lambda and network learning lambda both got same tiny lambda result, which should not be an occasional result;
   B. Grid Searching lambda on both Tongren and Duke_AMD data clearly demonstrated that muError is linear correlation with lambda; smaller lambda, smaller muError;
   C. Our theoretical analysis clearly demonstrated that adding lambda in cost function may improve error, may worse error; In total effect, lambda acts 0 effect.
      In other words, lambda may act as a positive or negative noise in different sample, therefore tiny lambda is network's natural choice;
      We also can explain that in proper data set, lambda may act a good effect; In other data set, lambda may act a negative effect;
   D. And we have used backward gradient, Cplex, manual computation 3 methods verifying that our IPM is correct;
   All there results from different dataset, from different methods, and from theory and experiments, all point to a tiny lambda.

# Nov 16th, 2020
Fine Tune Surface and Rift Subnet with fixed Lambda:
Exp config:  /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20201113A_FixLambda2Unet.yaml
Result:
1   fine Tune training didn't improve accuracy in validation test; (when lambda is small, this is possible)
2   result with fixed lambda on test data:
    config file: /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20201113A_FixLambda2Unet.yaml
    stdSurfaceError = tensor([2.4533, 1.0671, 1.6763], device='cuda:0')
    muSurfaceError = tensor([1.1122, 1.8906, 2.9410], device='cuda:0')
    stdError = 1.9619948863983154
    muError = 1.9812551736831665
    pixel number of violating surface-separation constraints: 0
3   compare with pure SurfaceSubnet result:
    test expDuke_20200902A_SurfaceSubnet.yaml experiment on test data:
    stdSurfaceError = tensor([2.4572, 1.0611, 1.7664], device='cuda:1')
    muSurfaceError = tensor([1.1576, 1.8849, 2.9892], device='cuda:1')
    stdError = 1.9898078441619873
    muError = 2.0106043815612793
4   use fixed small lambda, mu error improved 0.03 micrometer, while stdError improved 0.028 micrometer;

# Nov 13th, 2020
Professor ask 3 questions on initial grid search Lambda result on Duke_AMD data:
1) What is the method you used to get the error of 2.196 micrometer?
    2.196 is from pure surfaceSubent network with ReLU,without seperation rift as below.
    expDuke_20200902A_SurfaceSubnet.yaml experiment on validation data got:
    meanMuerror = 2.196 micrometer, stdError=2.171, mciepoch=74, learningRate=5e-3,
    surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141

 2) Compared to your previous experiments, why this dataset takes such a long time?
    1 Duke_AMD data is very big, 12.7 times bigger than Tongren data;
    2 As Tongren data is small, I load all training and validation data in memory at beginning, and use them for each epoch without read disk again;
      As Duke_AMD data is too big, we can not load all training and validation data in memory at beginning,therefore each epoch needs to read each file from disk;
      And SATA harddisk reading speed in our GPU Server is about 1000 times slower than memory access speed.

3) What are the relative magnitudes of the two terms in the objective function?
   In my Aug 29 email report, I computed 1/(2sigma^2) for Tongren data like below. In other words, 1/(2sigma^2) is at 0.1 magnitude .
    1/(2sigma^2) in different OCT data at about 0.1 magnitude should not change too much, as it is surface error variance against same Guassian GT with same sigma =20 in training.
    While our grid search lambda is 2.9e-7, very small.
    Its relative magnitude of 2 terms in the object function is 300K times.
    Duke_AMD experiment is same with Tongren experiment, grid searched lambda is very small, which is consistant with leanring lambda result.
    This result also is consist with our theoretical analysis: Lambda can not improve mu (maybe better, maybe worse), so a smaller lambda is the choice of the grid search, and also the choice of learning lambda.


Attached 0829 email report:
# compare 1/(2*sigma2) and lambda, which represent the magnitude of unary terms and pairwise terms in the cost function of IPM:
reciprocalTwoSigma2.shape = torch.Size([5, 9, 512])
mean of reciprocalTwoSigma2 = [1.7199, 0.8725, 0.0993, 0.2319, 0.9011, 0.5338, 0.0335, 0.2529, 1.7592]
min of reciprocalTwoSigma2 = [0.0334, 0.0288, 0.0193, 0.0106, 0.0186, 0.0059, 0.0060, 0.0185, 0.0345]
max of reciprocalTwoSigma2 = [2.7259, 1.6216, 0.1681, 0.4646, 1.3337, 1.1049, 0.0949, 0.3692, 3.4337]



# Nov 13th, Friday, 2020
1  test expDuke_20200902A_SurfaceSubnet.yaml experiment on test data:
    stdSurfaceError = tensor([2.4572, 1.0611, 1.7664], device='cuda:1')
    muSurfaceError = tensor([1.1576, 1.8849, 2.9892], device='cuda:1')
    stdError = 1.9898078441619873
    muError = 2.0106043815612793

2  output visual prediction result;  Done.
   all visul images at : /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images

3  using predicted R, and expDuke_20200902A_SurfaceSubnet.yaml to search a better lambda, from 1e-5 downward, on validation data:
   config file: /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20201109A_SearchLambda2Unet.yaml
   lambda search initial value: 1.0e-5, step: 1.0e-8; It search 1000 points.
   Result: at the search iteration 971, it get its minimum error
   nSearch,meanError,      meanStd,            lambda_0,               lambda_1,               surfErr_0,      surfErr_1,              surfErr_2,      surfStd_0,          surfStd_1,          surfStd_2,
   971,2.166304588317871,2.102299451828003,2.900840172515018e-07,2.900840172515018e-07, 0.9951231479644775,2.3246231079101562,3.1791670322418213, 1.3870776891708374,2.5179460048675537,1.637975811958313,

   the relation curve of meanError and Lambda: /home/hxie1/data/OCT_Duke/numpy_slices/log/SearchLambda2Unet/expDuke_20201109A_SearchLambda2Unet/testResult/searchLambda_replaceRwithGT_0_gridSearch_1e-5downward.png
   Comparing with case without lambda on validation data:
              meanError improve from 2.196 micrometer to 2.1663 micrometer;
              meanStd   improve from 2.171 micorometer to 2.102 micrometer;
   And lambda_0 = 2.900840172515018e-07, and lambda_1= 2.900840172515018e-07, very small.

4  using fixed lambda to fine tune Rift+SurfaceNet network:
   It has launched training, but very slow. 40 min per epoch.




5  use fixed lambda + fineTune network to get result on test data;



# Nov 10th, Tuesday.
Professor Wu point out: The duke data is from JHU's paper, it is not AMD data.  I need to find this data, may redo experiment.

# Nov 9th, Monday, 2020
Analyze previous experiment data:
1  expDuke_20200902A_SurfaceSubnet.yaml experiment on validation data got:
   meanMuerror = 2.196 micrometer, stdError=2.171, mciepoch=74, learningRate=5e-3,
   surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141

2  replace R with GT without smoothness, using expDuke_20200902A_SurfaceSubnet search Lambda on validaton data:
   nSearch	meanError	    meanStd	            lambda_0	        lambda_1	        surfErr_0	        surfErr_1	        surfErr_2	        surfStd_0	        surfStd_1	        surfStd_2
   99	2.16719126701355	2.09991383552551	9.9994576885365E-06	9.9994576885365E-06	0.996140241622925	2.31729507446289	3.18813920021057	1.39484310150146	2.49415946006775	1.6541827917099





# Sep 14th, Tuesday, 2020
Professor directed to use CPlex for lambda search to verify IPM again at convenient time.


# Sep 10th, Thursday, 2020
# Duke data grid search result:
Analysis:
1  Please refer to csv data and its relation between lambda and meanError;
2  Grid search uses un-smooth ground truth as r;
3  Duke data is 12.7 times bigger than Tongren data, and grid search is very time-consuming;
4  I searched 2 regions, each region needs about 16 hours with 3 GPUs;
   Region A: from 0.1 to 0.001, with grid step 0.001;
   Region B: from 0.001 to 1e-5, with grid step 1e-5;
5  Both searches show that it is basically a linear relation between meanError and lambda;
   Smaller lambda, better accuracy of IPM optimized surface location.
6  Recall the stationary condition formula like below of IPM cost function,
   it shows that the sum effect of mu and r uses lambda as weight to correct mu,
   mu and r occupy 50% weight respectively before multiplying lambda, and mu generally is far bigger than r.
   Therefore even perfect r from ground truth does not add benefit to correct mu,
   as this correcting process is dominated by mu/s self.


# Sep 4th, Friday, 2020
# on validation set with pixel resolution of 3.24 micrometer, at 11:30pm of Friday.
expDuke_20200902A_SurfaceSubnet: meanMuerror = 2.196 micrometer, epoch=74, learningRate=5e-3,
                                 surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141
                                 Now valiation loss is increasing.

expDuke_20200902A_RiftSubnet:    meanSepartionerror = 3.881 micrometer, epoch=101, learningRate=1.25e-3,
                                 riftError: rift0 = 3.089, rift1=4.673
                                 Now validation loss is still decreasing.


# Sep 2nd, 2020
1 Duke data is training:
  A   data set statistics:
  training set: 266 volumes x 51 slices per volume, where 187 AMD + 79 control = 266 volumes;
  validation set: 59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;
  test set:  59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;
  B   Now SurfaceSubset and SeparationSubset are in training separately, very slow, about 33 min/epoch;
      Duke data is about 12.7 times of Tongren data in the slices number: (266+59)*51/(42*31) = 12.7;
  C   After 32 epochs, surfaceSubset got mean surface error 2.6 micrometer in resolution of 3.24 micrometer per pixel;
      After 28 epochs, separationSubst got mean separation error 4.3 micrometer;
      both networks are still in training;
  D   At the moment, surface 0 get 1.4 micrometer, surface 1 gets 2.8 micrometer, surface 2 gets 3.7 micrometer errors;
      It shows the surface0 is very easy, while surface2 is harder to segment, which is led by the difference of different surface information;

2 Grid search-lambda script is ready, waiting the Duke network finishing pretraining;
  An intuitive analysis from below A and B may predict that our further grid search may still get a very small lambda.
  A  learning lambda also gets very small lambda  of 1e-11 level;
  B  Binary search: when lambda =0.01, muError =13; when lambda=0.0001, muError = 2.07 for Tongren data;
  As search-lambda network needs 3 GPUs in which SurfaceSubset and SeparationSubset run 2 GPUs, current GPU resource is busy on Duke.
  We need to wait for Duke to finish training, and then launch grid search lambda.

Plan:
1 As Duke data training is very slow, I plan to launch ovarian cancer project tomorrow;
2 When Duke data training finish, I will come back to continue Duke data work;

