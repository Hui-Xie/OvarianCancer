# Jan 6th, Wednesday, 2021
After U-Net, get CxHxW feature map, using 2D [H,1] convolution with pading [0,0] to get Nx1xW feature, here N is the number of thickness.

Define ThicknessSubnet_Y as yufanHe method plus 1D convolution.

Thickness (Rift) prediction on Duke_AMD test data:
=======================================================================================================================================================================================================
Method_Name                                         thicknessError          thicknessStdError   thickError0  thickError1  Note
expDuke_20200902A_RiftSubnet                        5.921670913696289       3.658905267715454   3.4042,      8.4392       Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007           3.5991899967193604      2.2252163887023926  3.0774,      4.1209       NoGuassian DivLoss, thickness from N surfaces, initial filter=32, 8layers
expDuke_20201215A_ThicknessSubnet_iibi007           3.800525188446045       2.357407331466675   3.2803,      4.3208       Gausssian DivLoss, thickness from N surfaces. initial filter=32, 8Layers
expDuke_20201228A_ThicknessSubnet_iibi007           3.8707656860351562      2.439850091934204   3.2910,      4.4505       without Gaussian Divloss, use cumulative R, intial filter=24. 7Layers
expDuke_20210101A_ThicknessSubnet_iibi007           3.7638235092163086      2.4115993976593018  3.1838,      4.3439       32 filters, 8 layers, add XY coordinate input, No cummulativeRLoss
expDuke_20210101B_ThicknessSubnet_iibi007           3.8175220489501953      2.0736165046691895  3.2453,      4.3898       32 filters, 8 layers, add XY coordinate input, with cumulativeRLoss
expDuke_20210107A_Thickness_YufanHePlus1D_iibi007   training, exact YufanHe network(XY channel+ layerSeg+ 64 filter for all layers) + 1D [H,1] convolution to predict thickness
expDuke_20210107B_Thickness_1D_iibi007              training, base on expDuke_20201209A network, but directly predict N-1 thickness, plus 1D [H,1] convoluiton.
========================================================================================================================================================================================================

Analysis:
1  The only difference between expDuke_20210101A and expDuke_20201209A is that 20210101A added XY coordinate channels.
   Now we can figure out the value-added point of JHU method is its layer segmentation branch with dice loss and cross entropy loss;
2  cumulative loss has value in improving thicknessError, but it improved std deviation;
3





# Jan 6th, Wednesday, 2021
expDuke_20210101A_ThicknessSubnet_iibi007:
      at 10:50 am, Jan 6th, it trained 164 epochs in 4 days 20hours.
      it current learning rate at 0.00625, which has not reached general 0.001
      and both validation loss and training loss are in slowly decline trend.
      I will wait one more day to check its result again.


expDuke_20210101B_ThicknessSubnet_iibi007:
      at 10:59 am, Jan 6th, it trained 172 epochs in 4 day 22 hours.
      its current learning rate at 0.003125,  which has not reached general 0.001
      and both validation loss and training loss are in slowly decline trend.
      I will wait one more day to check its result again.

Today, I will make the exact JHU method + a high (different with Leixin's width,as we detect thickness instead smooth)
convolution code ready.



# Jan 5th Tuesday, 2021:
Meetting with professor:
1  use total JHU method to predict thickness in SoftSeparation proejct;
2  try Leixin's idea of using wider context to get better thickness predicion;

# Jan 1st, Friday, 2020

Thickness (Rift) prediction on Duke_AMD test data:
=======================================================================================================================================================================================================
Method_Name                                 thicknessError          thicknessStdError   thicknessError0  thicknessError1    Note
expDuke_20200902A_RiftSubnet                5.921670913696289       3.658905267715454   3.4042,          8.4392             Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007   3.5991899967193604      2.2252163887023926  3.0774,          4.1209             NoGuassian DivLoss, thickness from N surfaces, initial filter=32, 8layers
expDuke_20201215A_ThicknessSubnet_iibi007   3.800525188446045       2.357407331466675   3.2803,          4.3208             Gausssian DivLoss, thickness from N surfaces. initial filter=32, 8Layers
expDuke_20201228A_ThicknessSubnet_iibi007   3.8707656860351562      2.439850091934204   3.2910,          4.4505             without Gaussian Divloss, use cumulative R, intial filter=24. 7Layers
expDuke_20210101A_ThicknessSubnet_iibi007   training     32 filters, 8 layers, add XY coordinate input, No cummulativeRLoss
expDuke_20210101B_ThicknessSubnet_iibi007   training     32 filters, 8 layers, add XY coordinate input, with CumulativeRLoss
========================================================================================================================================================================================================

Analysis:
1  The expDuke_20201228A didn't get improvement on muError:
   One possible reason is that it used the intial filter 24 and 7 layers which are same with the best Tongren network config;
2  further improvement:
   A introduce JHU's coordinates XY as 2 extra input channels;
   B increase intial filter =32 and 48, and 8 layers for 2 experiments;
   C both new experiment networks need 35 GB GPU memory;






# Dec 25th, Friday, 2020
Record the idea for further improving the thickness Network:
1  use cumulative R to compare with cumulative G with MSE;
2  it like compare 2 cumulative curves fitting;
3  It has some shadow of comparing 2 cumulative distributions.

MSE:  \sum (r_i-g_i)^2

new: \sum (\sum_{k=0}^{k} r_k - \sum_{k=0}^{k} g_k)^2  = ((r_0- g_0) + (r_1- g_1) +...+(r_k-g_k))^2

(r_k-g_k) ^2+ 2 *(r_m-g_m)(r_n-g_n)

expDuke_20201228A_ThicknessSubnet_iibi007:
improvements:
1  add cumulativeThickness Loss;
2  first layer filter =24, and 7 layers: which is the same with the best Tongren network;
3  dropout rate change into dropoutRateUnet: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]  # for nLayers=7;
4  it needs GPU memory 11863MB.







# Dec 24th, Thursday, 2020
previous trained 2 programs:
nohup python3 ./RiftSubnet_Train.py ./testConfig_Duke/expDuke_20201215A_ThicknessSubnet_iibi007.yaml &
nohup python3 ./SurfaceSubnet_Train.py ./testConfig_Duke/expDuke_20201215A_SurfaceSubnet_iibi007.yaml &

test programs:
nohup python3 ./RiftSubnet_Test.py ./testConfig_Duke/expDuke_20201215A_ThicknessSubnet_iibi007.yaml &
nohup python3 ./SurfaceSubnet_Test.py ./testConfig_Duke/expDuke_20201215A_SurfaceSubnet_iibi007.yaml &

check result:

Pure SurfaceSubnet and Thickness prediction Result on Dec 24th, 2020

Different methods on Duke_AMD test data for pure surface net, without softSeparation at all:
======================================================================================================================================================================================================
Method                      trainingGPUMemory   muError             stdError           Hausdorff_Distance(pixel)            expName
JHU(YufanHe)                18GB                1.9699755907058716  2.110976219177246  153.         40.         71.         expDuke_20201208A_SurfaceNet_YufanHe_iibi007, all layers have same filters=64
NoReLU_NoRift_ourMethod     10GB                2.0391054153442383  2.2191126346588135 65.          123.        54.         expDuke_20201117A_SurfaceSubnet_NoReLU with start filters=24
ReLU_NoRift_OurMethod(24)   10GB                1.9812164306640625  1.961887240409851  50.          33.         30.         expDuke_20200902A_SurfaceSubnet with start filters=24
ReLU_NoRift_OurMethod(64)   45GB                2.003045082092285   2.13523006439209   63.          60.         57.         expDuke_20201215A_SurfaceSubnet_iibi007 with start filters=64
======================================================================================================================================================================================================

AMD and Control group statistics for surfaceSubnet on test data:
========================================================================================================================================================
ExpName                                         Group       muError             stdError            muSurfaceError              Hausdorff_Distance(pixel)
--------------------------------------------------------------------------------------------------------------------------------------------------------
expDuke_20201208A_SurfaceNet_YufanHe_iibi007    AMD         2.1859207153320312  2.406925916671753   1.3248, 2.0069, 3.2261      153.    40.     71.
expDuke_20201208A_SurfaceNet_YufanHe_iibi007    Control     1.478100061416626   1.0525134801864624  0.7013, 1.6296, 2.1034      92.     15.     13.
--------------------------------------------------------------------------------------------------------------------------------------------------------
expDuke_20200902A_SurfaceSubnet                 AMD         2.202664852142334   2.2394497394561768  1.3282, 1.9938, 3.2859      50.     33.     30.
expDuke_20200902A_SurfaceSubnet                 Control     1.4768062829971313  0.929665207862854   0.6202, 1.6550, 2.1552      28.     8.      8.
--------------------------------------------------------------------------------------------------------------------------------------------------------
expDuke_20201215A_SurfaceSubnet_iibi007         AMD         2.243525505065918   2.453505039215088   1.3835, 1.9919, 3.3552      63.     29.     57.
expDuke_20201215A_SurfaceSubnet_iibi007         Control     1.4552843570709229  0.9183998107910156  0.5611, 1.6485, 2.1562      19.     60.     9.
========================================================================================================================================================

Thickness (Rift) prediction on Duke_AMD test data:
=====================================================================================================================================================================================================
Method_Name                                 thicknessError          thicknessStdError   thicknessErrorRift0     thicknessErrorRift1      Note
expDuke_20200902A_RiftSubnet                5.921670913696289       3.658905267715454   3.4042,                 8.4392                   Directly prediction N-1 thickness.
expDuke_20201209A_ThicknessSubnet_iibi007   3.5991899967193604      2.2252163887023926  3.0774,                 4.1209                   without Guassian surface DivLoss, get thickness from N surfaces
xpDuke_20201215A_ThicknessSubnet_iibi007    3.800525188446045       2.357407331466675   3.2803,                 4.3208                   with Gausssian surface DivLoss, get thickness from N surfaces
=====================================================================================================================================================================================================

Analysis:
1  our latest experiments in blue:
   expDuke_20201215A_ThicknessSubnet_iibi007.yaml: uses 8 days 5 hours for training 285 epochs.
   expDuke_20201215A_SurfaceSubnet_iibi007.yaml:  uses 8 days 5hours for training 140 epochs.
2  For pure surfaceNet:
   A. it looks adding more filters does not help;
   B. new 1215 experiment has better result in Control group than 1208 experiment, but AMD group worse;
3  For Thickness prediction:
   A. it looks adding guassian N-surface divLoss odes not help;
   B. further improvement: try to introduce (N-1) thickness, intead of surface, gaussian divLoss;


#python3 ./computeAMD_ControlAccuracy.py
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201215A_SurfaceSubnet_iibi007/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([3.4635, 1.0393, 1.7873])
muSurfaceError = tensor([1.3835, 1.9919, 3.3552])
HausdorffDistance in pixel = [[63. 29. 57.]]
HausdorffDistance in physical size (micrometer) = [[204.12  93.96 184.68]]
stdError = 2.453505039215088
muError = 2.243525505065918
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.2939, 0.9639, 0.4560])
muSurfaceError = tensor([0.5611, 1.6485, 2.1562])
HausdorffDistance in pixel = [[19. 60.  9.]]
HausdorffDistance in physical size (micrometer) = [[ 61.56 194.4   29.16]]
stdError = 0.9183998107910156
muError = 1.4552843570709229
===============


python3 ./computeAMD_ControlAccuracy.py
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfacesUnet_YufanHe_2/expDuke_20201208A_SurfaceNet_YufanHe_iibi007/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([3.0343, 1.7488, 1.8714])
muSurfaceError = tensor([1.3248, 2.0069, 3.2261])
HausdorffDistance in pixel = [[153.  40.  71.]]
HausdorffDistance in physical size (micrometer) = [[495.72 129.6  230.04]]
stdError = 2.406925916671753
muError = 2.1859207153320312
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([1.0478, 1.0402, 0.4431])
muSurfaceError = tensor([0.7013, 1.6296, 2.1034])
HausdorffDistance in pixel = [[92. 15. 13.]]
HausdorffDistance in physical size (micrometer) = [[298.08  48.6   42.12]]
stdError = 1.0525134801864624
muError = 1.478100061416626
===============




# Dec 21th, Monday, 2020
python3 ./RiftSubnet_Train.py ./testConfig_Duke/expDuke_20201215A_ThicknessSubnet_iibi007.yaml:
A  further improve the thickness prediciton accuracy by adding gaussian location loss;
B  it uses 35GB GPU memory;
C  It has trained 5 days for 174 epoch,  5*24*60/174= 42 min/epoch;
D  It may need one more day to get final stable status;

=================================================================================================================
python3 ./SurfaceSubnet_Train.py ./testConfig_Duke/expDuke_20201215A_SurfaceSubnet_iibi007.yaml:
A  It adds filter number of ReLU solution to get better muError;
B  It uses 45GB GPU memory;
C  It has trained 4d 22hours for 84 epochs: (4*24*60+22*60)/84  =  84 min /epoch; It is still in training;
D  this netowrk increases filters at the begining layer from 24 to 64, and other layer same. It is almost 2.7 times bigger of 20200901A_SurfaceSubnet work;
E  Currently it just trained 84 epochs and learning just reduce one once by reduceOnPlateau , and it still needs about 3 days;


# Dec 15th, Tuesday, 2020
Meeting with professor:
1   add filter number of ReLU solution, to get better muError;  --45GB GPU memory, done; Next Tuesday get result;
2   further improve the thickness prediciton accuracy;         -- add gaussian location loss, 35GB GPU memory, done; Next Tuesday get result;

# Result of using pure SurfaceSubnet with ReLU without softSeparation:
Experiment Name: expDuke_20200902A_SurfaceSubnet
Testtime: output_20201214_201624
=====================================================================
Total Result without differentiating AMD and Control:
stdSurfaceError = tensor([2.4530, 1.0670, 1.6765], device='cuda:3')
muSurfaceError = tensor([1.1122, 1.8904, 2.9410], device='cuda:3')
stdError = 1.961887240409851
muError = 1.9812164306640625
hausdorff Distance = [[50. 33. 30.]]
pixel number of violating surface-separation constraints: 0
=====================================================================
=====================================================================
Result on AMD and Control Group:
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
========================================================================
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9140, 1.0875, 1.8950])
muSurfaceError = tensor([1.3282, 1.9938, 3.2859])
HausdorffDistance in pixel = [[50. 33. 30.]]
HausdorffDistance in physical size (micrometer) = [[162.   106.92  97.2 ]]
stdError = 2.2394497394561768
muError = 2.202664852142334
===================================================================
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4255, 1.0083, 0.4461])
muSurfaceError = tensor([0.6202, 1.6550, 2.1552])
HausdorffDistance in pixel = [[28.  8.  8.]]
HausdorffDistance in physical size (micrometer) = [[90.72 25.92 25.92]]
stdError = 0.929665207862854
muError = 1.4768062829971313
===========================================================================





# Dec 14th, Monday, 2020
JHU method result on Duke_AMD validation data: (18GB memory for training)
expName: expDuke_20201208A_SurfaceNet_YufanHe_iibi007
best error at epoch 47
muError: 2.11
muErroSurface: 0.9226, 2.266, 3.14
muStd:   2.084
muStdSurface: 1.37, 2.346, 1.804

JHU method on test Duke_AMD test data:
expName: expDuke_20201208A_SurfaceNet_YufanHe_iibi007
stdSurfaceError = tensor([2.5991, 1.5675, 1.6567], device='cuda:1')
muSurfaceError = tensor([1.1345, 1.8918, 2.8836], device='cuda:1')
stdError = 2.110976219177246
muError = 1.9699755907058716
hausdorff Distance = [[153.  40.  71.]]
pixel number of violating surface-separation constraints: 0

Different methods on Duke_AMD test data for pure surface net, without softSeparation at all:
============================================================================================================================================================================
Method                      trainingGPUMemory   muError             stdError           Hausdorff_Distance(pixel)            expName
JHU                         18GB                1.9699755907058716  2.110976219177246  153.         40.         71.         expDuke_20201208A_SurfaceNet_YufanHe_iibi007
NoReLU_NoRift_ourMethod     10GB                2.0391054153442383  2.2191126346588135 65.          123.        54.         expDuke_20201117A_SurfaceSubnet_NoReLU (newTest)
ReLU_NoRift_OurMethod       10GB                1.9812164306640625  1.961887240409851  50.          33.         30.         expDuke_20200902A_SurfaceSubnet (newTest)
=============================================================================================================================================================================


==================================================================================

cmd: python3 ./RiftSubnet_Train.py ./testConfig_Duke/expDuke_20201209A_ThicknessSubnet_iibi007.yaml
New ThicknessNetwork for  on Duke_AMD validation data: (34GB memory for trainig)
expName: expDuke_20201209A_ThicknessSubnet_iibi007
best error at epoch 112
muError: 4.036
muErroSurface: 3.495, 4.646
muStd:   2.725
muStdSurface: 2.663, 2.689


comparing with history:
RiftSubnet For test data:
experimentName:expDuke_20200902A_RiftSubnet: for Rift prediction:
muError = 5.921670913696289
muSurfaceError = tensor([3.4042, 8.4392], device='cuda:2')
stdError = 3.658905267715454
stdSurfaceError = tensor([2.7749, 2.5322], device='cuda:2')
Lr = 1.0e-2
epoch = 42  for training.

===========================================
===============Formal Output Result ===========
experimentName:expDuke_20201209A_ThicknessSubnet_iibi007
stdThicknessError = tensor([2.5041, 1.7789], device='cuda:0')
muThicknessError = tensor([3.0774, 4.1209], device='cuda:0')
stdError = 2.2252163887023926
muError = 3.5991899967193604
hausdorff distance of Thickness = [[45.939484 49.970154]]

%%%%%%%%%%%%%%%%%%%%%%%%%

Thickness (Rift) prediction on Duke_AMD test data:
====================================================================================================================================
Method_Name                                 thicknessError          thicknessStdError   thicknessErrorRift0     thicknessErrorRift1
expDuke_20200902A_RiftSubnet                5.921670913696289       3.658905267715454   3.4042,                 8.4392
expDuke_20201209A_ThicknessSubnet_iibi007   3.5991899967193604      2.2252163887023926  3.0774,                 4.1209
=====================================================================================================================================

Analysis:
1  current ThicknessPrediction Network(expDuke_20201209A_ThicknessSubnet_iibi007) didn't use surfaceLocation loss;
2  further improvement: add surfacelocation gaussian loss;
3  risk: adding surfaceLocation guasssian loss may improve muError, but it may reduce differentiating information from surfacenet;
         but it ThicknessNetwork has different network architecture, it may be still has differentiating information;






# Dec 9th, Wednesday, 2020
Further think the improvement of RiftNet:
idea of further improving rift(thickness) network:
  A  Inherit the technologies of our current U-net predicting N surfaces with high accuracy;
  B  First use a separate U-Net to output N-surface location probabilities, and then compute its (N-1) thickness(R) from this N-Surfaces location probability;
  C  Compare this (N-1) thickness with ground truth R to compute loss and backward propagation;
  D  This network only uses (N-1) thickness ground truth, without using N surface ground truth at all;
     As ground truths and loss functions, network initialization are not same, riftNetwork and surfaceNetwork are different networks.
  F  This method fits physical intuition of layer thickness computation;
  G  Add dropout in the UNet After concatenation in the expand path;   --done
  H  Add filter number of each layer, as predicting thickness is more complicated than predicting surfaces;  --done
  I  Thickness ground truth does not need smooth;   --done
  G  Use L1 loss + smoothThicknessLoss; --done
  I  initial layer filter number =32, which is different with surfaceNet;  --done

Now new thickness network  in training.

Duke_AMD data is 12.7 times bigger than Tongren data,
and we can not load all of them into memory one time, so each epoch, we need load data from the disk.
and both new network has bigger filter number.
The new JHU network and Thickness Network both need 45 min per epoch.

Duke_AMD data statistics:
  training set: 266 volumes x 51 slices per volume, where 187 AMD + 79 control = 266 volumes;
  validation set: 59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;
  test set:  59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;



# Dec 07th, Monday, 2020
The result of grid searching lambda for gaussian noise + perfect thickness(R) on validation data:

Summary:
1 Unconstrained soft separation optimization + ReLU do improves muError; and its training speed is very quick, comparing with IPM;
2 Higer accuracy of thickness prediction, bigger lambda values, less muError;
3 From the below Guassian noise + perfect R experiments comparing with NoReLU, No optimization,
  it improves Hausdorff distance significantly.
4 idea of further improving rift(thickness) network:
  A  Inherit the technologies of our current U-net predicting N surfaces with high accuracy;
  B  First use a separate U-Net to output N-surface location probabilities, and then compute its (N-1) thickness(R) from this N-Surfaces location probability;
  C  Compare this (N-1) thickness with ground truth R to compute loss and backward propagation;
  D  This network only uses (N-1) thickness ground truth, without using N surface ground truth at all;
     As ground truths and loss functions are not same, riftNetwork and surfaceNetwork are different networks.
  F  This method fits physical intuition of layer thickness computation;


Some tabular details:
HausdorffDistance in pixel for NoReLU and No optimization = [[38.474457 66.437805 68.57501 ]]
HausdorffDistance in physical size (micrometer) for NoReLU and No optimization = [[124.65724 215.25848 222.18304]]
muError at NoReLU and No optimiztion (lambda=0) = 2.383660316467285
at lambda0_min= 0, and lambda1_min = 0, with ReLU, muError = 2.383739471435547

# below the best result of grid searching lambda for guassian noise + perfect thickness(rift) on validation data:
# blank means no measurement in that programs.
-------------------------------------------------------------------------------------------------------------------------
RiftError           muError                 HausdorffDistance(in pixel)     Lambda_0 Lambda_1
-------------------------------------------------------------------------------------------------------------------------
0.0                 1.6399389505386353                                      2.06     3.99   (this perfect R case)
2.197073459625244   1.90798819065094    49.196      52.69426    39.68515    0.1      0.25   (gaussian noise + perfect R)
2.4565892219543457  1.9342775344848633  49.261276   53.30031    37.779327   0.08     0.2    (gaussian noise + perfect R)
2.7137765884399414  1.9585803747177124  48.728424   54.165863   35.518845   0.07     0.16   (gaussian noise + perfect R)
2.972208023071289   1.9796805381774902  49.18547    54.445984   34.13138    0.06     0.14   (gaussian noise + perfect R)
5.921670913696289   2.3492398262023926                                      0.004    0.004  (current predicted R case)
NoRift/No Optimize  2.383660316467285   38.474457   66.437805   68.57501    0.0      0.0    (No ReLU, No optimization)
-------------------------------------------------------------------------------------------------------------------------

# Dec 4th, Friday, 2020
Ideas for further improving R:
1  Use dense network architecture;
2  Use U-Net to predict N surface, and then get N-1 layer thickness to compare with ground truth;


# Dec 2nd, Wednesday, 2020
Goal:
   Before improving the RiftSubnet, Let's add gaussian noises to perfect ground truth R, to see what kind of accuracy we can get with grid search lambda:
Design:
1  add 4 different gaussian noises to perfect ground truth R, to search its best lambda separately;
2  alos measure haudorff distance in grid search;   -- done

Subtasks:
1  add gaussian noise, generate its numpy array and save to harddisk file;  --done
python3 ./generateGaussianNoiseGTRift.py
Gaussian noise sigma = 0.85 for below rift(thickness) error:
	stdRiftError = tensor([0.0127, 0.0122], device='cuda:0')
	RiftError = tensor([2.1980, 2.1961], device='cuda:0')
	stdRiftError = 0.012410839088261127
	riftError = 2.197073459625244
========================
Gaussian noise sigma = 0.95 for below rift(thickness) error:
	stdRiftError = tensor([0.0156, 0.0115], device='cuda:0')
	RiftError = tensor([2.4568, 2.4564], device='cuda:0')
	stdRiftError = 0.013624467886984348
	riftError = 2.4565892219543457
========================
Gaussian noise sigma = 1.05 for below rift(thickness) error:
	stdRiftError = tensor([0.0178, 0.0169], device='cuda:0')
	RiftError = tensor([2.7154, 2.7121], device='cuda:0')
	stdRiftError = 0.01734689623117447
	riftError = 2.7137765884399414
========================
Gaussian noise sigma = 1.15 for below rift(thickness) error:
	stdRiftError = tensor([0.0197, 0.0172], device='cuda:0')
	RiftError = tensor([2.9714, 2.9730], device='cuda:0')
	stdRiftError = 0.01843925192952156
	riftError = 2.972208023071289
========================

2  modifiy main program to support noiseRiftGT to search 4 ranges:  --Runing. wait to result.
   lambda0_min, lambda0_max, lambda0_step = 0, 5.0, 1.0e-2
   lambda0_min, lambda0_max, lambda0_step = 0, 5.0e-1, 2.0e-3
   lambda0_min, lambda0_max, lambda0_step = 0, 4.0e-3, 2.0e-5
   lambda0_min, lambda0_max, lambda0_step = 0, 4.0e-7, 2.0e-9










# Nov 30th, Monday, 2020
Grid search Lambda with unconstrained optimziaiton +ReLU on the validation data:
Summary:
1  at lambda=0 (which means not using soft separation) on validation data, muError = 2.38373947;
2  In the case of using perfect R from ground truth, lambda0=2.06, lambda1=3.99, muError=1.6399;
   Reflecction:
   A  The coefficient unary terms is in [0.000017, 3.492] with mean 0.4896; does this big lambda make sense in physical meaning?
   B  If lambda0= 0.004, lambda1=0.004 at predicted R case level, the muError = 2.24765 with the perfect R, improving 5.7% comparing with  no softSeparation;
   C  Previous binary search experiments didn't search so big lambda value.
3  In the case of using predicted R from another network, lambda0=0.004, lambda1=0.004, muError = 2.349234;
   Comparing with the No soft separation case, this muError improves 1.4%;
   Reflection:
   A when R is not accurate, improvement is not explict;
   B We need to further improve predict R; we need to rethink the architecture of predictR network to improve it;
   C A caveat: The accuracy of R is more difficult to achieve the same level with mu,as R involves 2 measured surfaces;
4  Searching lambda with constrained IPM, and searching Lambada with unconstrained optimization may have different behaviors;

Grid search Lambda with unconstrained optimziaiton +ReLU:
sigma2Path = /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201117A_SurfaceSubnet_NoReLU/testResult/validation/validation_simga2.npy
muPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201117A_SurfaceSubnet_NoReLU/testResult/validation/validation_mu.npy
rPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/RiftSubnet/expDuke_20200902A_RiftSubnet/testResult/validation/validation_Rift.npy
gPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201117A_SurfaceSubnet_NoReLU/testResult/validation/validation_gt.npy
riftGTPath = /home/hxie1/data/OCT_Duke/numpy_slices/log/RiftSubnet/expDuke_20200902A_RiftSubnet/testResult/validation/validation_RiftGts.npy
the coefficient of unary terms(Q = 1.0/(sigma^2)):
Q/2 min = 0.000174444867298007
Q/2 mean = 0.48962265253067017
Q/2 max = 3.4920814037323
output Dir = /home/hxie1/data/OCT_Duke/numpy_slices/searchSoftLambda

at both min lambdas (=0), MuError= 2.383739471435547
===============================
Use predicted R for grid-searching Lambda:

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4.0, 0.01
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4.0, 0.01
min error location with mu error = 2.356391668319702:
axis x: location: x= 1, lambda0 = 0.01
axis y: location: y= 1, lambda1 = 0.01

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 0.5, 0.002
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 0.5, 0.002
min error location with mu error = 2.3492398262023926: (Best)
axis x: location: x= 2, lambda0 = 0.004
axis y: location: y= 2, lambda1 = 0.004

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 0.004, 2e-05
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 0.004, 2e-05
min error location with mu error = 2.3492515087127686:
axis x: location: x= 199, lambda0 = 0.00398
axis y: location: y= 199, lambda1 = 0.00398

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4e-05, 2e-07
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4e-05, 2e-07
min error location with mu error = 2.3812191486358643:
axis x: location: x= 200, lambda0 = 3.9999999999999996e-05
axis y: location: y= 200, lambda1 = 3.9999999999999996e-05

rSource = predictR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4e-07, 2e-09
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4e-07, 2e-09
min error location with mu error = 2.383711099624634:
axis x: location: x= 190, lambda0 = 3.8e-07
axis y: location: y= 198, lambda1 = 3.96e-07

==============================
Use perfect ground truth R for grid-searching Lambda:

rSource = GTR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4.0, 0.01
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4.0, 0.01
min error location with mu error = 1.6399389505386353:  (Best)
axis x: location: x= 206, lambda0 = 2.06
axis y: location: y= 399, lambda1 = 3.99

rSource = GTR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 0.5, 0.002
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 0.5, 0.002
min error location with mu error = 1.695436954498291:
axis x: location: x= 249, lambda0 = 0.498
axis y: location: y= 249, lambda1 = 0.498

rSource = GTR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 0.004, 2e-05
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 0.004, 2e-05
min error location with mu error = 2.247654676437378:
axis x: location: x= 199, lambda0 = 0.00398
axis y: location: y= 199, lambda1 = 0.00398

rSource = GTR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4e-05, 2e-07
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4e-05, 2e-07
min error location with mu error = 2.3794524669647217:
axis x: location: x= 200, lambda0 = 3.9999999999999996e-05
axis y: location: y= 200, lambda1 = 3.9999999999999996e-05

rSource = GTR
axis x: lambda0_min, lambda0_max, lambda0_step = 0, 4e-07, 2e-09
axis y: lambda1_min, lambda1_max, lambda1_step = 0, 4e-07, 2e-09
min error location with mu error = 2.383693218231201:
axis x: location: x= 198, lambda0 = 3.96e-07
axis y: location: y= 198, lambda1 = 3.96e-07

=================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# Nov 26th, 2020: Unary terms coefficient:
Q = 1/(sigma^2) on validation data:
Q/2 min = 0.000174444867298007
Q/2 mean = 0.48962265253067017
Q/2 max = 3.4920814037323

# Nov 25th, 2020  expDuke_20201117A_SurfaceSubnet_NoReLU Result
expDuke_20201117A_SurfaceSubnet_NoReLU finished:
1  trained 372 epochs in 7day 21hour 12min:
   (7*24*60+21*60+12)/372 = 30.5 min / epoch;
2  best validation error:
   at epoch 85: with learning rate 5.0e-3
   meanError: 2.207
                Error      Std
   surface 0:   0.9427     1.259
   surface 1:   2.371      2.588
   surface 2:   3.306      1.814
3  test on test data:
=======net running parameters=========
B,S,H,W = (3009, 3, 512, 361)
Test time: 204.82324028015137 seconds.
net.m_runParametersDict:
	validationLoss:40.726619720458984
	epoch:76
	errorMean:2.205378532409668
	learningRate: 5.0e-3

===============Formal Output Result ===========
patientIDList =[]
stdSurfaceError = tensor([3.0907, 1.1151, 1.5977], device='cuda:3')
muSurfaceError = tensor([1.2110, 1.9339, 2.9723], device='cuda:3')
stdError = 2.21911358833313
muError = 2.039097547531128
pixel number of violating surface-separation constraints: 67
slice number of violating surface-separation constraints: 5
slice list of violating surface-separation constraints:
	/home/hxie1/data/OCT_Duke/numpy_slices/test/AMD_1134_images_s29.npy
	/home/hxie1/data/OCT_Duke/numpy_slices/test/AMD_1257_images_s39.npy
	/home/hxie1/data/OCT_Duke/numpy_slices/test/AMD_1257_images_s41.npy
	/home/hxie1/data/OCT_Duke/numpy_slices/test/AMD_1257_images_s42.npy
	/home/hxie1/data/OCT_Duke/numpy_slices/test/AMD_1105_images_s44.npy
==================================================
test on Divided AMd and Control Group without ReLU:
python3.7 ./computeAMD_ControlAccuracy.py
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201117A_SurfaceSubnet_NoReLU/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([3.6814, 1.1590, 1.7918])
muSurfaceError = tensor([1.4751, 2.0601, 3.3228])
HausdorffDistance in pixel = [[ 65. 123.  54.]]
HausdorffDistance in physical size (micrometer) = [[210.6  398.52 174.96]]
stdError = 2.5565528869628906
muError = 2.2859814167022705
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.3893, 0.9779, 0.4277])
muSurfaceError = tensor([0.6097, 1.6466, 2.1740])
HausdorffDistance in pixel = [[46. 13.  9.]]
HausdorffDistance in physical size (micrometer) = [[149.04  42.12  29.16]]
stdError = 0.9188244938850403
muError = 1.4767508506774902
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
RiftSubnet For test data:
experimentName:expDuke_20200902A_RiftSubnet: for Rift prediction:
stdSurfaceError = tensor([2.7749, 2.5322], device='cuda:2')
muSurfaceError = tensor([3.4042, 8.4392], device='cuda:2')
stdError = 3.658905267715454
muError = 5.921670913696289

Lr = 1.0e-2
epoch = 42  for training.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Search best Lambda on validation data with unconstrained soft separation:
1  use riftNet: "/home/hxie1/Projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20200902A_RiftSubnet.yaml"
   and SurfaceNet Without ReLU:expDuke_20201117A_SurfaceSubnet_NoReLU
2  basic design:
   A use unconstrained softSeparation and ReLU at the last to estimate Lambda;
   B use unconstrained softSeparation optimal solution get S;
   C use validation data, without augmentation, output all its Q, mu, r:
     Q: Bx51x3x361
     mu:Bx51x3x361
     r: Bx51x2x361,
     g: Bx51x3x361,
     where B is the number of OCT volumes of validation data;
3  some subtasks:
   A generate Q,mu,r,g in validation data; -- done;
      surfaceResult: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20201117A_SurfaceSubnet_NoReLU/testResult/validation
      riftResult: /home/hxie1/data/OCT_Duke/numpy_slices/log/RiftSubnet/expDuke_20200902A_RiftSubnet/testResult/validation
      And checked its order are consistent.
   B code the unconstrained optimal function;  --done
   C grid search lambda with generated Q,mu, r,g on validation data; -- on going. Monday it will have initial result.
     2 experiments parallel both on validation data:
     A  use predicted R;
     B  use ground truth R;
     1 search needs one second.
      lambda0_min, lambda0_max, lambda0_step = 0, 4.0, 0.01
      lambda1_min, lambda1_max, lambda1_step = 0, 4.0, 0.01,
      grid search totally needs 400*400 seconds = 44.4 hours. On Monday it will have initial result.

   D test expDuke_20201117A_SurfaceSubnet_NoReLU on test data;  --done, with average error 2.039 um.;
   E test searched fixed lambda fine tune on validation data with 2 experiments;   -- plan on Monday
      A  fixed Lambda + unconstrained soft separation optimization+ ReLU;
      B  fixed Lambda + IPM;
   F test searched fixed lambda final on test data with 2 experiments;             --plan on Wednesday
      A  fixed Lambda + unconstrained soft separation optimization + ReLU;
      B  fixed Lambda + IPM;

# Nov 19th, 2020
Exact slices with max hausdorff distance on Duke_AMD test data.

Summary:
1   These slices represent the worst error in the test data;
    Note: Hausdorff distance is a measurement relating to specific cases.
2   check error one by one:
    AMD_1198_images_s24_GT_Predict:  a rare disease case that surface 0 broken in the fovea region;
    AMD_1127_images_s23_GT_Predict:  surface 1 prediction error. By the way: the ground truth of surface 0 is not good;
    AMD_1198_images_s17_GT_Predict:  surface 2 prediction error;
    Control_1051_images_s16_GT_Predict: surface 0 signal is blurry, leading to prediction error;
    Control_1005_images_s24_GT_Predict: surface 1 prediction error;
    Control_1016_images_s08_GT_Predict: explicit ground truth error;
3   Surface 0 has maximum Hausdorff distance because the AMD_11198 case in the test data is a rare ILM broken patient;
4   Please refer to the attachments.

Search the worst cases: the slices with max hausdorff distance.
######################################################
Use softSeparation + fixed lambda network on Test data:
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
HausdorffDistance in pixel = [[50. 33. 31.]]
HausdorffDistance in physical size (micrometer) = [[162.   106.92 100.44]]
surface 0: the location of hausdorff at w=179 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/AMD_1198_images_s24_GT_Predict.png
surface 1: the location of hausdorff at w=126 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/AMD_1127_images_s23_GT_Predict.png
surface 2: the location of hausdorff at w=341 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/AMD_1198_images_s17_GT_Predict.png
===============
GroupName: Control
case number = 18
HausdorffDistance in pixel = [[28.  8.  8.]]
HausdorffDistance in physical size (micrometer) = [[90.72 25.92 25.92]]
surface 0: the location of hausdorff at w=71 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/Control_1051_images_s16_GT_Predict.png
surface 1: the location of hausdorff at w=171 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/Control_1005_images_s24_GT_Predict.png
surface 2: the location of hausdorff at w=132 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/images/Control_1016_images_s08_GT_Predict.png
===============
######################################################
Use Unet + ReLU network on Test data
python3.7 ./findMaxHausdorffPoint.py
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
HausdorffDistance in pixel = [[50. 31. 44.]]
HausdorffDistance in physical size (micrometer) = [[162.   100.44 142.56]]
surface 0: the location of hausdorff at w=179 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/AMD_1198_images_s24_GT_Predict.png
surface 1: the location of hausdorff at w=360 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/AMD_1100_images_s48_GT_Predict.png
surface 2: the location of hausdorff at w=360 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/AMD_1100_images_s48_GT_Predict.png
===============
GroupName: Control
case number = 18
HausdorffDistance in pixel = [[33.  9.  8.]]
HausdorffDistance in physical size (micrometer) = [[106.92  29.16  25.92]]
surface 0: the location of hausdorff at w=48 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/Control_1051_images_s17_GT_Predict.png
surface 1: the location of hausdorff at w=171 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/Control_1005_images_s24_GT_Predict.png
surface 2: the location of hausdorff at w=132 of path: /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images/Control_1016_images_s08_GT_Predict.png
===============
######################################################


# Nov 19th, 2020
Compute Hausdorff distance and segmentation error computation on AMD and Control test data separately.
Summary:
Compare pure Unet+ReLu and SoftSeparation Network:
1  In AMD test data, surface 2 improves about 30% of Hausdorff distance; while surface 0 didn't improve, and surface 1 worsen a little;
   We can use this surface 2 as an example for a demonstration;
2  In Control test data, surface 0 and surface 1 improve a little, while surface 3 didn't change.
   The reason behind it maybe is that control test data already has a high accuracy; (High accuracy improves small);
3  Hausdorff distance improves really depending on data.
4  In total result, SoftSeparation improves both Hausdorff distance, and mean accuracy;
5  Unet without ReLU is still in training. After its training, we may further compare it with SoftSeparation network;
   This comparison will be more explicit in our estimation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately in SoftSeparation Network + fixed Lambda
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9144, 1.0877, 1.8948])
muSurfaceError = tensor([1.3283, 1.9940, 3.2859])
HausdorffDistance in pixel = [[50. 33. 31.]]
HausdorffDistance in physical size (micrometer) = [[162.   106.92 100.44]]
stdError = 2.239591121673584
muError = 2.202744960784912
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4251, 1.0081, 0.4460])
muSurfaceError = tensor([0.6201, 1.6549, 2.1552])
HausdorffDistance in pixel = [[28.  8.  8.]]
HausdorffDistance in physical size (micrometer) = [[90.72 25.92 25.92]]
stdError = 0.9295611381530762
muError = 1.4767508506774902
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately in Unet + ReLU
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9164, 1.0329, 2.0026])
muSurfaceError = tensor([1.3764, 1.9611, 3.3444])
HausdorffDistance in pixel = [[50. 31. 44.]]
HausdorffDistance in physical size (micrometer) = [[162.   100.44 142.56]]
stdError = 2.267122268676758
muError = 2.2273151874542236
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4579, 1.1336, 0.4604])
muSurfaceError = tensor([0.6594, 1.7114, 2.1802])
HausdorffDistance in pixel = [[33.  9.  8.]]
HausdorffDistance in physical size (micrometer) = [[106.92  29.16  25.92]]
stdError = 0.9794862270355225
muError = 1.516985535621643
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


# Nov 17th, 2020
Compute Accuracy separately in AMD and Control of Duke_AMD data:
Analysis:
1  Comparing ReLU constraints with SoftSepartion constraints, control group accuracy improves more with SoftSeparation;
2  Control group improves 0.04, while AMD group improves 0.02;
   So I have some suspicion that more disease data may will get better result.
3  Comparing our result with Leixin's result on the Table 1 of https://arxiv.org/pdf/2007.01217.pdf;
   our surface 3 corresponds with OBM, our surface 2 correpondis with IRPE. Our Control correspond its normal.
   Both our result in both control and AMD groups are better than Leixin's best result "W/".
   Leixin's used neighbor smooth constraints, while our method used soft separation constraints.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately in expDuke_20201113A_FixLambda2Unet Network:
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9144, 1.0877, 1.8948])
muSurfaceError = tensor([1.3283, 1.9940, 3.2859])
stdError = 2.239591121673584
muError = 2.202744960784912
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4251, 1.0081, 0.4460])
muSurfaceError = tensor([0.6201, 1.6549, 2.1552])
stdError = 0.9295611381530762
muError = 1.4767508506774902
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately in pure SurfaceSubnet with ReLU constraint without SoftSeparation:
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9164, 1.0329, 2.0026])
muSurfaceError = tensor([1.3764, 1.9611, 3.3444])
stdError = 2.267122268676758
muError = 2.2273151874542236
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4579, 1.1336, 0.4604])
muSurfaceError = tensor([0.6594, 1.7114, 2.1802])
stdError = 0.9794862270355225
muError = 1.516985535621643
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# Nov 17th, 2020
Meeting with professor:
1  we need to use JHU method to test Duke_AMD data;
2  make sure JHU method same with its paper, without using our gradient;
3  JHU's method has a Journel verson;

My think:
1  Leixin's method is single surface optimization, so its lambda only fit its single surface;
2  Our method is multi-surface paralllel optimization, so its final lambda is the balance of multi-surface;
3  we may need a special disease for show our method;
4  soft separation cost function does not consider the lambda for top and bottom surfaces.

# Nov 16th, 2020
Response to professor Wu:
 "I'd suggest you to calculate the segmentation accuracy for the normal and AMD subjects separately. I am still puzzling on the tiny \lambda."
 "We should compare to the model without any separation constraints. Also please compute the segmentation accuracy for the normal and AMD subjects separately.

1  OK.  I will write a script to divide normal and AMD case, and statistics its accuracy separately in test set;
2  "Compare model without any separation constraints" is a wonderful/insightful idea; Thank you,professor. ReLU is a hard constraint.
    OK. I need to retrain a network to test it;
3  I am totally not surprise this tiny lambda result:
   A. Grid searching lambda and network learning lambda both got same tiny lambda result, which should not be an occasional result;
   B. Grid Searching lambda on both Tongren and Duke_AMD data clearly demonstrated that muError is linear correlation with lambda; smaller lambda, smaller muError;
   C. Our theoretical analysis clearly demonstrated that adding lambda in cost function may improve error, may worse error; In total effect, lambda acts 0 effect.
      In other words, lambda may act as a positive or negative noise in different sample, therefore tiny lambda is network's natural choice;
      We also can explain that in proper data set, lambda may act a good effect; In other data set, lambda may act a negative effect;
   D. And we have used backward gradient, Cplex, manual computation 3 methods verifying that our IPM is correct;
   All there results from different dataset, from different methods, and from theory and experiments, all point to a tiny lambda.

# Nov 16th, 2020
Fine Tune Surface and Rift Subnet with fixed Lambda:
Exp config:  /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20201113A_FixLambda2Unet.yaml
Result:
1   fine Tune training didn't improve accuracy in validation test; (when lambda is small, this is possible)
2   result with fixed lambda on test data:
    config file: /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20201113A_FixLambda2Unet.yaml
    stdSurfaceError = tensor([2.4533, 1.0671, 1.6763], device='cuda:0')
    muSurfaceError = tensor([1.1122, 1.8906, 2.9410], device='cuda:0')
    stdError = 1.9619948863983154
    muError = 1.9812551736831665
    pixel number of violating surface-separation constraints: 0
3   compare with pure SurfaceSubnet result:
    test expDuke_20200902A_SurfaceSubnet.yaml experiment on test data:
    stdSurfaceError = tensor([2.4572, 1.0611, 1.7664], device='cuda:1')
    muSurfaceError = tensor([1.1576, 1.8849, 2.9892], device='cuda:1')
    stdError = 1.9898078441619873
    muError = 2.0106043815612793
4   use fixed small lambda, mu error improved 0.03 micrometer, while stdError improved 0.028 micrometer;

# Nov 13th, 2020
Professor ask 3 questions on initial grid search Lambda result on Duke_AMD data:
1) What is the method you used to get the error of 2.196 micrometer?
    2.196 is from pure surfaceSubent network with ReLU,without seperation rift as below.
    expDuke_20200902A_SurfaceSubnet.yaml experiment on validation data got:
    meanMuerror = 2.196 micrometer, stdError=2.171, mciepoch=74, learningRate=5e-3,
    surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141

 2) Compared to your previous experiments, why this dataset takes such a long time?
    1 Duke_AMD data is very big, 12.7 times bigger than Tongren data;
    2 As Tongren data is small, I load all training and validation data in memory at beginning, and use them for each epoch without read disk again;
      As Duke_AMD data is too big, we can not load all training and validation data in memory at beginning,therefore each epoch needs to read each file from disk;
      And SATA harddisk reading speed in our GPU Server is about 1000 times slower than memory access speed.

3) What are the relative magnitudes of the two terms in the objective function?
   In my Aug 29 email report, I computed 1/(2sigma^2) for Tongren data like below. In other words, 1/(2sigma^2) is at 0.1 magnitude .
    1/(2sigma^2) in different OCT data at about 0.1 magnitude should not change too much, as it is surface error variance against same Guassian GT with same sigma =20 in training.
    While our grid search lambda is 2.9e-7, very small.
    Its relative magnitude of 2 terms in the object function is 300K times.
    Duke_AMD experiment is same with Tongren experiment, grid searched lambda is very small, which is consistant with leanring lambda result.
    This result also is consist with our theoretical analysis: Lambda can not improve mu (maybe better, maybe worse), so a smaller lambda is the choice of the grid search, and also the choice of learning lambda.


Attached 0829 email report:
# compare 1/(2*sigma2) and lambda, which represent the magnitude of unary terms and pairwise terms in the cost function of IPM:
reciprocalTwoSigma2.shape = torch.Size([5, 9, 512])
mean of reciprocalTwoSigma2 = [1.7199, 0.8725, 0.0993, 0.2319, 0.9011, 0.5338, 0.0335, 0.2529, 1.7592]
min of reciprocalTwoSigma2 = [0.0334, 0.0288, 0.0193, 0.0106, 0.0186, 0.0059, 0.0060, 0.0185, 0.0345]
max of reciprocalTwoSigma2 = [2.7259, 1.6216, 0.1681, 0.4646, 1.3337, 1.1049, 0.0949, 0.3692, 3.4337]



# Nov 13th, Friday, 2020
1  test expDuke_20200902A_SurfaceSubnet.yaml experiment on test data:
    stdSurfaceError = tensor([2.4572, 1.0611, 1.7664], device='cuda:1')
    muSurfaceError = tensor([1.1576, 1.8849, 2.9892], device='cuda:1')
    stdError = 1.9898078441619873
    muError = 2.0106043815612793

2  output visual prediction result;  Done.
   all visul images at : /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images

3  using predicted R, and expDuke_20200902A_SurfaceSubnet.yaml to search a better lambda, from 1e-5 downward, on validation data:
   config file: /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20201109A_SearchLambda2Unet.yaml
   lambda search initial value: 1.0e-5, step: 1.0e-8; It search 1000 points.
   Result: at the search iteration 971, it get its minimum error
   nSearch,meanError,      meanStd,            lambda_0,               lambda_1,               surfErr_0,      surfErr_1,              surfErr_2,      surfStd_0,          surfStd_1,          surfStd_2,
   971,2.166304588317871,2.102299451828003,2.900840172515018e-07,2.900840172515018e-07, 0.9951231479644775,2.3246231079101562,3.1791670322418213, 1.3870776891708374,2.5179460048675537,1.637975811958313,

   the relation curve of meanError and Lambda: /home/hxie1/data/OCT_Duke/numpy_slices/log/SearchLambda2Unet/expDuke_20201109A_SearchLambda2Unet/testResult/searchLambda_replaceRwithGT_0_gridSearch_1e-5downward.png
   Comparing with case without lambda on validation data:
              meanError improve from 2.196 micrometer to 2.1663 micrometer;
              meanStd   improve from 2.171 micorometer to 2.102 micrometer;
   And lambda_0 = 2.900840172515018e-07, and lambda_1= 2.900840172515018e-07, very small.

4  using fixed lambda to fine tune Rift+SurfaceNet network:
   It has launched training, but very slow. 40 min per epoch.




5  use fixed lambda + fineTune network to get result on test data;



# Nov 10th, Tuesday.
Professor Wu point out: The duke data is from JHU's paper, it is not AMD data.  I need to find this data, may redo experiment.

# Nov 9th, Monday, 2020
Analyze previous experiment data:
1  expDuke_20200902A_SurfaceSubnet.yaml experiment on validation data got:
   meanMuerror = 2.196 micrometer, stdError=2.171, mciepoch=74, learningRate=5e-3,
   surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141

2  replace R with GT without smoothness, using expDuke_20200902A_SurfaceSubnet search Lambda on validaton data:
   nSearch	meanError	    meanStd	            lambda_0	        lambda_1	        surfErr_0	        surfErr_1	        surfErr_2	        surfStd_0	        surfStd_1	        surfStd_2
   99	2.16719126701355	2.09991383552551	9.9994576885365E-06	9.9994576885365E-06	0.996140241622925	2.31729507446289	3.18813920021057	1.39484310150146	2.49415946006775	1.6541827917099





# Sep 14th, Tuesday, 2020
Professor directed to use CPlex for lambda search to verify IPM again at convenient time.


# Sep 10th, Thursday, 2020
# Duke data grid search result:
Analysis:
1  Please refer to csv data and its relation between lambda and meanError;
2  Grid search uses un-smooth ground truth as r;
3  Duke data is 12.7 times bigger than Tongren data, and grid search is very time-consuming;
4  I searched 2 regions, each region needs about 16 hours with 3 GPUs;
   Region A: from 0.1 to 0.001, with grid step 0.001;
   Region B: from 0.001 to 1e-5, with grid step 1e-5;
5  Both searches show that it is basically a linear relation between meanError and lambda;
   Smaller lambda, better accuracy of IPM optimized surface location.
6  Recall the stationary condition formula like below of IPM cost function,
   it shows that the sum effect of mu and r uses lambda as weight to correct mu,
   mu and r occupy 50% weight respectively before multiplying lambda, and mu generally is far bigger than r.
   Therefore even perfect r from ground truth does not add benefit to correct mu,
   as this correcting process is dominated by mu/s self.


# Sep 4th, Friday, 2020
# on validation set with pixel resolution of 3.24 micrometer, at 11:30pm of Friday.
expDuke_20200902A_SurfaceSubnet: meanMuerror = 2.196 micrometer, epoch=74, learningRate=5e-3,
                                 surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141
                                 Now valiation loss is increasing.

expDuke_20200902A_RiftSubnet:    meanSepartionerror = 3.881 micrometer, epoch=101, learningRate=1.25e-3,
                                 riftError: rift0 = 3.089, rift1=4.673
                                 Now validation loss is still decreasing.


# Sep 2nd, 2020
1 Duke data is training:
  A   data set statistics:
  training set: 266 volumes x 51 slices per volume, where 187 AMD + 79 control = 266 volumes;
  validation set: 59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;
  test set:  59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;
  B   Now SurfaceSubset and SeparationSubset are in training separately, very slow, about 33 min/epoch;
      Duke data is about 12.7 times of Tongren data in the slices number: (266+59)*51/(42*31) = 12.7;
  C   After 32 epochs, surfaceSubset got mean surface error 2.6 micrometer in resolution of 3.24 micrometer per pixel;
      After 28 epochs, separationSubst got mean separation error 4.3 micrometer;
      both networks are still in training;
  D   At the moment, surface 0 get 1.4 micrometer, surface 1 gets 2.8 micrometer, surface 2 gets 3.7 micrometer errors;
      It shows the surface0 is very easy, while surface2 is harder to segment, which is led by the difference of different surface information;

2 Grid search-lambda script is ready, waiting the Duke network finishing pretraining;
  An intuitive analysis from below A and B may predict that our further grid search may still get a very small lambda.
  A  learning lambda also gets very small lambda  of 1e-11 level;
  B  Binary search: when lambda =0.01, muError =13; when lambda=0.0001, muError = 2.07 for Tongren data;
  As search-lambda network needs 3 GPUs in which SurfaceSubset and SeparationSubset run 2 GPUs, current GPU resource is busy on Duke.
  We need to wait for Duke to finish training, and then launch grid search lambda.

Plan:
1 As Duke data training is very slow, I plan to launch ovarian cancer project tomorrow;
2 When Duke data training finish, I will come back to continue Duke data work;

