# surfaceUnet + thicknessUnet+ learning Lamba
# For SoftSeparationNet D: no NxW learning lambda, directly learn alpha.

debug: False # debug mark
dataIn1Parcel: False
useIndependentValidation: True
# Duke data is not in one parcel, while Tongren, JHU are in one parcel.
# dataIn1Parcel means each patient has a volume file.
dataInSlice: True

status: "trainLambda" # only 3 status: # "trainLambda" #"test"  "fineTune"
# test: pure test, no any backward propagation
# fineTune: all networks support backward propagation
# trainLambda: only lambda module uses backward propagation.


dataDir: "/localscratch/Users/hxie1/data/OCT_Duke/numpy_slices"
existGTLabel: True
# log will save at: dataDir + "/log/" + network + "/" + experimentName
K: 0  # Kfold cross validation
k: -1   # the fold k test
batchSize: 4
groundTruthInteger: True

network: "SoftSeparationNet_D"
device: torch.device('cuda:1')   #GPU ID
sigma: 0.0  # For gaussian  ground truth, in float, 0 means use dynamic Sigma

# some physical parameter of images
inputHeight: 512   # original height
inputWidth: 361   # rawImageWidth
slicesPerPatient: 51  # for test and validation set
hPixelSize: 3.24  # unit: micrometer, in y/height direction
numSurfaces: 3  # num of surfaces in an image
gradChannels: 7   # the added gradient channels beside raw image channel
bothSideGrad: True # False use singleSideGrad
addCoordinatesXYChannels: True
useRift: True
smoothRift: False  # smooth Rift in ground truth
scaleNumerator: 1
scaleDenominator: 1
useLayerDice: True
segChannels: 64  # number of channel in the segmentation part.

careLambdaLossOnly: True  #  True: only backward lambdaLoss; False: backward 3 losses.

# data augmentation
augmentProb: 0.4  #  data augmentation rate
gaussianNoiseStd: 0.1 # gausssian nosie std with mean =0
# for salt pepper noise
saltPepperRate: 0.05  # rate = (salt+pepper)/allPixels
saltRate: 0.5  # saltRate = salt/(salt+pepper)
rotation: False
flippingProb: 0.3
TTA: False       #TestTime Augmentation
TTA_StepDegree: 0 # roration step degree of TTA, in integer degree
lacingWidth: 0  # Lace the both end of 0 or 360 degree to offset inaccurate segementation at boundary of input image


# for 2 Unets + learning Lambda
surfaceSubnet: "SurfaceSubnet_Q"
surfaceSubnetYaml: "/localscratch/Users/hxie1/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20210507A_SurfaceSubnetQ128_iibi007.yaml"
surfaceSubnetDevice: torch.device('cuda:1')   #GPU ID
surfaceSubnetLr: 0.1  # pretain at epoch 26  # in test status, it does not care
surfaceSubnetLrReset: False

thicknessSubnet: "ThicknessSubnet_Q"
# for a new trained network.
thicknessSubnetYaml: "/localscratch/Users/hxie1/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20210507_ThicknessQ64_iibi007.yaml"
thicknessSubnetDevice: torch.device('cuda:1')   #GPU ID
thicknessSubnetLr: 0.1  # pretrain at epoch 37   # in test status, it does not care
thicknessSubnetLrReset: False

lambdaModule: "LambdaModule_F"
lambdaModuleYaml: ""
lambdaModuleDevice: torch.device('cuda:1')   #GPU ID
lambdaModuleLr: 0.1
lambdaModuleLrReset: True
# alpha: 0.001  # use learning alpha deduced from lambda
maxAlpha: 0.1
lambdaOutputC: 2  # output Channel.

netPath: "/localscratch/Users/hxie1/data/OCT_Duke/numpy_slices/netParameters"  # net is saved at netpath / network / self_filename
loadNetPath: ""
outputDir: ""
logDir: "/localscratch/Users/hxie1/data/OCT_Duke/numpy_slices/log"
refXMLFile: "/localscratch/Users/hxie1/data/OCT_Tongren/refXML/1062_OD_9512_Volume_Sequence_Surfaces_Iowa.xml"
