# Jun 8th, 2021
share dir:

\\garvinlab.storage.iibi.uiowa.edu\garvinlab_prof_wu


# June 8th, Dicom convert:
Summary:

·         Basically, my program can access Zeiss' dicom files and read the tags in these files

·         However, there so many types of files in their system and the not all the tags can be found in all the files.

·         Currently, I rename the files to the format that human can read rather than their internal coding system

·         For some image files, I still can't directly access the pixel values. I will work with Hui and see how to deal with them.

·         Following are the files that I can convert [and access the data]:

o    .PDF

·         Cirrus_OU_ONH and RNFL OU Analysis.pdf

·         Guided Progression Analysis.pdf

·         Macular Thickness Analysis.pdf

·         OU Ganglion Cell OU Analysis.pdf

·         HD 5 Line Raster.pdf

·         HD 21 Line.pdf

·         HD Radial.pdf



·         Visual Field.pdf

·         GOLDMANN VISUAL FIELDM.pdf

·         COMBINED_REPORT_24-2_RNFL.pdf

·         GPAL3F.pdf

·         GPA_SUMMARY_SITA_STANDARD.pdf

·         SFA.pdf



·         All_Color_Fundus_Photo.pdf



o    .PNG

·         LSO.png [Line Scanning Ophthalmoscope]

·         ColorFundus.png



o    .RAW

·         Macular-Cube-200x200.raw

·         HD-5-Line-Raster.raw

·         5-Line-Raster.raw

·         RASTER_RADIAL.raw

·         RASTER_21_LINES.raw





·         Following are the format that I need to work with Hui and see if we can figure out [otherwise, we may need help from Zeiss] [maybe for some of them, we have .pdf and these are just their raw data format.]

o    HfaVisualFieldRawData

o    HfaPerimetryOphthalmicPhotography8BitImage



o    CapeCodGuidedProgressionAnalysisRawData

o    HfaOphthalmicVisualFieldStaticPerimetryMeasurements



o    CapeCodOpticDiscCubeRawData

o    CapeCodOpticDiscAnalysisRawData



o    CapeCodMacularCubeAnalysisRawData

o    CapeCodMacularCubeRawData


# June 2nd, Wednesday, 2021
Meeting time: 10am -10:40 am June 2nd, Wednesday, 2021
Attendee:  Jui Kai, Hui
Topic: About the thin retina sementation
Minutes:
1  After getting the initial ground truth corrected by Ray from graph search result, we need to do:
   A rescaling image to uniform dimension;
   B flip all OS eyes into OD eyes;
   C a slight smooth the ground truth before using.
2  Note: in the deep learning augmentaion, do not use flip as the nasal and temporal direction is not symmtric in OD/OS eye.

Ray's comments:
Notes for manual tracing:
Surface 0, 1, 3, 6, 10 in the surface xml file → I will add surface 5 soon
My tracing: at the end of the xml filename with "_Ray"
Graph search: the regular filename
Try to read the OCT image and match the surfaces in your own system

A "very gentle" 3D smoothing process (or thin-plate-spline) should be applied to reduce the manual tracing artifact
Check the smoothing results again in the images to make sure they still look reasonable
Make sure the top surface of GCIPL (i.e., surface_1) is NOT above ILM (surface_0)

I also added some scans with acceptable graph search results
Flip the OS to the OD orientation
It's acceptable (doesn't have to) that the ILM and GCIPL surfaces touch each other at the fovea
It's also acceptable that ILM and top GCIPL surface very close to each other



# May 26th, Wednesday, 2021
Meeting time: 10am -10:40 am May 26th, Wednesday, 2021
Attendee:  Jui Kai, Hui
Topic: About Dicom converter and ground truth.
Minutes:
1  For the Cirrus HD-OCT 6000 instrument, we need to decode all Dicom information to mhd/raw format or pdf, not just the OCT volume.
   There are some risks that the manufacture may encrypt the dicom data in order to sell its value-added software.
   We will try to decode the format. Hui will do this in the first week of June.
2  Ray will generate the ground truth of 5 patients for the VIP OCT data in the first week of June.
   HUi will use the 5 ground truth to generate 25 initial ground truth by deep learning network, and then Ray will manually modify it.
3  The current OCT viewer needs to add support multi-Bscan browse function.  Hui will help Ray to modify it.




# May 22nd, Saturday, 2021
1  Use Slicer on X11 forwarding. use ssh -X instead of ssh -Y option.  --done.
2  convert Dicom to Nrrd.
3  write a script of Nrrd read program.
4  connect vpn.healthcare.uiowa.edu.

/home/hxie1/data/Ophthalmology/3Dicom/Test/Orig_Output_from_Zeiss/01082671/2017-02-13   : Macular cube 200x200_2   




# May 21th, Friday, 2021
Dicom file analysis:
1  there are 3 patient Dicom.
2  Each patient has 4-5 visit records in different dates.
3  Each date has 4 volumes OCT volumes, but these 4 OCT volumes has same content. (We may need to save one volue, need to confirm?)
4  OCT volume size: 200x1024x200 in #AScans x PenetrationDepth x #Bscans format.
5  Convert each date record as a Raw file or Nrrd file?
6  Save a volume or a series separate Bscan?







# May 19th, Wednesday, 2021
Meeting time: 10am -10:40 am May 19th, Wednesday, 2021
Attendee:  Jui Kai, Hui
Topic: About BMO prediction and VAE project
Minutes:
1  The project using VAE to detect BMO:
    A  It has 3 surfaces each B-scan, 3 segmented B-scan per patient, total has 100+ patients.
    B  Segmentation of Less-one pixel accuracy is not very important as ground truth self may has errors bigger than one pixel.
       A reasonable result even with 3-5 pixels error is more important.
       Ray: Yes, a reliable segmentation globally is the focus here; so far, these layers are more important:
       RNFL, GCIPL, RPE complex, maybe also include the lower bounding surface of the OPL [i.e., Surface: 0, 1, 3, 5, 6, 10]

    C  For the manual segmentation tool, we need further improve its UI function for easier using.
    D  After the improving manual segmentation tools, a NewYork doctor may help us to get more ground truth.
2  For the thin retina project:
    A  Jui will offer segmentation ground truth for 5 patients.
    B  Basing on this 5 patient ground truth, we will use an initial segmentation network to generate more ground truth for further manual correction.
    C  Getting the thickness percent of diseased patients vs normal patients is more important publication.
3  Jui will give some Dicom images with irregular axis orientation to Hui, and then Hui help convert them into raw format for further use.


# May 12th, Wednesday, 2021
Meeting time: 10am -11am May 12th, Wednesday, 2021
Attendee:  Jui Kai, Hui
Topic: About BMO prediction and VAE project
Minutes:
1  BMO prediction project:
   We can first use 2D Bscans as input to predict the middle-broken low boundaries of RPE complex.
   The 2 opening points of BMO are usually corresponding around the swelling peak on ILM, which can give some information to help prediction.
   Each patient may have 3 Bscans for our data.
2  Use VAE to detect segmentation map.
   The output of VAE is not the recovery image as the traditional VAE, but the low dimension surface segmentation.
   The final goal of this project is to manipulate the latent vector to get different surface shape, which expresses the disease manifestations.

1 In the thin RNFL case, there is still some boundary information of low boundary of the thin RNFL.
2 Some time, the Ophthalmologist needs to use other Bscan information to infer the surface boundary of the current Bscan.
3 The dataset for glaucoma:
  diseased cases: about 25, control case: about 25,   with size of 200x200x1024, where X: #Ascans, Y:#Bscans, Z, Penetration depth.
  number of surfaces: about 5.
  OCT may have different dimension size, we need to unify them into one same size.
4 further thinking: Use volume OCT image with VAE to detect Bruch membrane opening (BMO).


#May 11th, research VAE
1  Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) arr both generative models.
   a VAE is an autoencoder whose encodings distribution is regularised during the training in order to
   ensure that its latent space has good properties allowing us to generate some new data.

autoencoders cannot be used to generate new data and will introduce Variational Autoencoders that are
 regularised versions of autoencoders making the generative process possible.

Encoder extracts/compresses feature into encoded space or latent space,
 which is a kind of loosy dimensionality reduction.

 the autoencoder is solely trained to encode and decode
  with as few loss as possible, no matter how the latent space is organised.

  satisfying this way the expected continuity and completeness conditions.
  Naturally, as for any regularisation term, this comes at the price of a higher reconstruction error
   on the training data




#May 5th, Wednesday, 2021:
Meeting time: 10am -11am May 5th, Wednesday, 2021
Attendee:  Jui Kai, Hui
Topic: about the OCT segmentation tool.
Minutes:
1 In the thin RNFL case, there is still some boundary information of low boundary of the thin RNFL.
2 Some time, the Ophthalmologist needs to use other Bscan information to infer the surface boundary of the current Bscan.
3 The dataset for glaucoma:
  diseased cases: about 25, control case: about 25,   with size of 200x200x1024, where X: #Ascans, Y:#Bscans, Z, Penetration depth.
  number of surfaces: about 5.
  OCT may have different dimension size, we need to unify them into one same size.
4 further thinking: Use volume OCT image with VAE to detect Bruch membrane opening (BMO).


# April 29th,Thursday, 2021:
Questions needing discussions:
1  I want to know the knowledge and logic from the OCT image which leads you to determine this is a thin RNFL, and the surface RNFL/GCL is very close to or overlaps with the surface ILM.
   If we can express this knowledge and logic as a model or constraint into our network, it may get better accuracy and a more valuable paper.

2   And what are other characteristics of this special disease OCT images?

3   A possible way is to involve a prior knowledge about the normal subject range:
    If this is a case, do we need to add normal/control OCT images with ground truth to help network
    distinguish normal and disease OCT Images.


# April 28th, Wednesday, 2021


Meeting: Wu, Ray, Hui
Details of the segmentation method, including setting up ground truth, network structure, ect..
IRB
Working load/hours/updates
Potential conferences that we can aim for


# Ray email on April 23rd, 2021:
Current issue:
• Currently, the layer segmentation is done by the graph-search algorithm. For most of the healthy
subjects, the macular scans can be segmented correctly. However, for some glaucoma subjects,
due to damage of the optic nerve, the retinal nerve fiber layer (RNFL) and ganglion cell + inner
plexiform layer (GCIPL) start to get thinning [please see the figures above]. Due to the graph-
search inter-surface constrains, the algorithm has a good chance to miss the "invisible" RNFL and
sees the thinning GCIPL as a part of the unreal thick RNFL. Consequently, the GCIPL segmentation
outcome gets to push down to another layer.

Goal:
• We first would like to develop a relatively straightforward deep-learning-based tool to overcome
this segmentation problem for these thin retinas. We hope that the deep learning neural network
can label interested layers correctly and robustly. To make this problem more manageable, we
currently only needs to segment a few layers/surfaces that contains the key information that we
want to measure [e.g., the GCIPL, the internal limiting membrane (ILM), the lower surface of the
outer plexiform layer and the RPE complex. Furthermore, we would like the developing deep
neural network could have the ability to output a projected-view pixel-based layer segmentation
confidence map. So, for a large amount of scan processing, we could only focus on a small portion
of images that need attention for quality control.